[
  {
    "objectID": "index.html#fa-hand-peace-hi-im-david.-im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project-to-work-on",
    "href": "index.html#fa-hand-peace-hi-im-david.-im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project-to-work-on",
    "title": "munoztd0",
    "section": " Hi, I’m David. I’m a Data Scientist / R Developer always looking for a cool open-source project to work on !",
    "text": "Hi, I’m David. I’m a Data Scientist / R Developer always looking for a cool open-source project to work on !"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Blog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR and Python side-by-side for data wrangling\n\n\n\n\n\n\nR\n\n\nPython\n\n\ntidyverse\n\n\npolars\n\n\nplotly\n\n\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n\n\n\n\n\n\nHow to finally learn to code in 7 easy steps\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 7, 2024\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nMy problem with Python\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 7, 2024\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nThe polyglot programmer: mastering multiple languages for optimal solutions\n\n\n\n\n\n\nR\n\n\nPython\n\n\nJulia\n\n\nNim\n\n\nJavaScript\n\n\n\n\n\n\n\n\n\nDec 17, 2023\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nExciting Insights from the 2023 Posit conference\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nwebr\n\n\nconference\n\n\n\n\n\n\n\n\n\nDec 17, 2023\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nThe two-minute rule for busy coders/learners\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 17, 2023\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nFree programming exercice websites\n\n\n\n\n\n\nR\n\n\nPython\n\n\nC\n\n\nC++\n\n\nNim\n\n\nHaskell\n\n\nOCaml\n\n\nJavaScript\n\n\nHTML/CSS\n\n\nSQL\n\n\nLua\n\n\nComputer Science\n\n\nInteractive Exercises\n\n\n\n\n\n\n\n\n\nNov 26, 2023\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nA Computer Science Roadmap\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nR Road Map\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nYou must use projects with RStudio!\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nContributing to projects on GitHub\n\n\n\n\n\n\nComputer Science\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nYour first chat bot with python!\n\n\n\n\n\n\nPython\n\n\nAI\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n\n\n\n\n\n\nYour first chat bot with python!\n\n\n\n\n\n\nPython\n\n\nAI\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n\n\n\n\n\n\nHow to contribute to our website with blog posts using R and RStudio\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nData Viz with Echarts4r\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 25, 2023\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n\n\n\n\n\n\nR Lunches in university of Geneva\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n\n\n\n\n\n\nWe-Data Live on YouTube\n\n\n\n\n\n\nYouTube\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\nDavid Munoz Tord, Fabrice Hategekimana and Vestin Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation with R\n\n\n\n\n\n\nR\n\n\nInteractive Exercises\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n\n\n\n\n\n\nWhat a tool! - An ignorant perspective\n\n\n\n\n\n\nComputer Science\n\n\nYouTube\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nFabrice Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nPython Basics\n\n\n\n\n\n\nPython\n\n\nYouTube\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\nFabrice Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nData Exploration with R\n\n\n\n\n\n\nR\n\n\nInteractive Exercises\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n\n\n\n\n\n\nSatistics with R\n\n\n\n\n\n\nR\n\n\nYouTube\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nFabrice Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nCode Hygiene. Don’t Laugh it off !\n\n\n\n\n\n\nComputer Science\n\n\nYouTube\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nFabrice Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nLearn Terminal Basics\n\n\n\n\n\n\nBash\n\n\nYouTube\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\nFabrice Hategekimana\n\n\n\n\n\n\n\n\n\n\n\n\nData Viz Fundamentals with R\n\n\n\n\n\n\nR\n\n\nInteractive Exercises\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n\n\n\n\n\n\nLearn R Basics\n\n\n\n\n\n\nR\n\n\nYouTube\n\n\n\n\n\n\n\n\n\nJun 1, 2022\n\n\nFabrice Hategekimana\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/2023-04-Deploy-your-R-code/index.html",
    "href": "talks/2023-04-Deploy-your-R-code/index.html",
    "title": "Deploy Your R Code!",
    "section": "",
    "text": "Full Slides\nPresentation Recording"
  },
  {
    "objectID": "talks copy/2018-09-ucla-cpi/index.html",
    "href": "talks copy/2018-09-ucla-cpi/index.html",
    "title": "Thoughts and Perspectives on PSH from the Ground Up",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "talks copy/2017-09-Data-and-Donuts/index.html",
    "href": "talks copy/2017-09-Data-and-Donuts/index.html",
    "title": "Ex-nihilo Data Analysis; Getting up and Running with Limited Resources",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "blog/Data_explor_Fundamentals/index.html",
    "href": "blog/Data_explor_Fundamentals/index.html",
    "title": "Data Exploration with R",
    "section": "",
    "text": "Goal\nLearn about data exploration and familiarize yourself with some of the basic functions of the tidyverse.\nOpen it full\nNote: All in english"
  },
  {
    "objectID": "blog/R-Lunches/index.html",
    "href": "blog/R-Lunches/index.html",
    "title": "R Lunches in university of Geneva",
    "section": "",
    "text": "R lunches are multidisciplinary meetings on R at UniMail.\nWe finished this semester R Lunches but you can still find the video links if you missed one!\nRead more about it"
  },
  {
    "objectID": "blog/R_basics/index.html",
    "href": "blog/R_basics/index.html",
    "title": "Learn R Basics",
    "section": "",
    "text": "By Vestin Hategekimana\nGoal: To initiate people who would like to start with R by starting with the basics or the basics or people who want to reinforce their knowledge by going through important concepts in programming.\nPlaylist here\nNote: video in french, ask in comments for subtitle in your language"
  },
  {
    "objectID": "blog/R_basics/index.html#first-vidéo-of-the-playlist",
    "href": "blog/R_basics/index.html#first-vidéo-of-the-playlist",
    "title": "Learn R Basics",
    "section": "First vidéo of the playlist:",
    "text": "First vidéo of the playlist:"
  },
  {
    "objectID": "blog/Data_manip_Fundamental/index.html",
    "href": "blog/Data_manip_Fundamental/index.html",
    "title": "Data Manipulation with R",
    "section": "",
    "text": "Goal\nLearn more about data manipulation: how to pivot, join and filter data using {dplyr} and {tidyr} packages.\nOpen it full\nNote: All in english"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\n\nEcharts4r\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2022\n\n\nDavid Munoz Tord\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Talks\n\n\n\n\n\n\n\n\n\n\n\nDeploy Your R Code!\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nDavid Munoz Tord\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "vitae copy.html",
    "href": "vitae copy.html",
    "title": "Full CV",
    "section": "",
    "text": "I believe that the 21st century requires a more multifaceted toolkit if we hope to solve complex challenges–process and task oriented approaches balanced within a framework that fosters strategic thinking. That interdisciplinary relationships can be strengthened through active community participation–working toward something with the shared value of doing good.\n I bring a lot of energy and enthusiasm to the projects that I become involved in. I thrive in situations that require imagination and innovation. My interests are varied; among my passions I would list: academic inquiry, classical and contemporary thought, the music and cuisine of other cultures, the arts, traveling, languages, and social justice. I believe that our most vulnerable deserve respect and to be treated with dignity and fairness.\n Ultimately I hope that the work I do is helpful–that it enables discovery or provides clarity. I hope to bring together practical and principled approaches and I hope to further grow as this philosophy guides me.\n\n\n\n\n\n\n I am dedicated and hard working–a good communicator and active community participant. I work well on team efforts and projects with the ultimate aim of using data for social good.\n I am:\n\n creative\n honest\n friendly\n enthusiastic\n keen to learn new things and take on new roles\n reliable\n a lateral thinker\n motivated\n\n I am able to:\n\n speak and read French\n read and understand some German\n listen to and follow instructions accurately\n work cooperatively\n take on new challenges\n complete tasks\n support others\n identify new innovations and technologies while working toward implementing them\n be proactive\n\n\n\n\n\n\n\n Web Technologies: HTML, CSS, Javascript; Quarto publishing; Rmarkdown Websites, Dashboards in R with shiny/flexdashboard that support crosstalk, and interactive data visualization with Plotly; Static Site Generators, i.e., Pelican, Jekyll, and others; Traditional CMSs like Wordpress and Drupal; previous coursework in Information Architecture and UX/UI design.\n Coding: R, Python, and Javascript.\n OSs: macOS, Linux, Windows, and containers. Within the Linux ecosystem, I have experience with Debian based distributions as well as Red Hat Enterprise Linux. I love the terminal and wish vim bindings were universal.\n Data: Relational Databases (data modeling and SQL); Extract-Transfer-Load (ETL) and Extract-Load-Transfer (ELT) strategies for data engineering–specifically with Apache Airflow for ETL and Data Build Tool (dbt) for ELT; information retrieval; taxonomies and ontologies; and, knowledge organization theory.\n Data Science: both R and Python are a big part of my workflow from reading in data, transforming and ‘tidying’ data, exploring and visualizing data, modeling, reporting, presenting, and tying it all together in a dashboard. In connection to R, I am a proponent of the tidyverse framework for its unifying API. With respect to Python, I enjoy using polars, pyjanitor, pymc, and plotly to work in equivalent ways. I’m honestly happy working in both languages.\n Metadata schemas: knowledge of RDF, OWL, SPARQL, AACR2, RDA, MODS, METS, and PREMIS\n\n\n\n\n\n\n University of North Texas | Master of Science in Information Science (2013) Information Systems track\n University of California, Los Angeles (UCLA) | Bachelor of Arts in French and comparative literature (2008 / double major)\n Université de la Sorbonne Nouvelle (Paris III) | Student exchange (2006–2007)\n\n\n\n\n\n\n\n\n\n\nexcited to bring my DevOps and Data Engineering skills to bear on thorny data problems in support of public good.\n\n\nSkylight is a digital government consultancy driven by making government work in a digital world through design, technology, and procurement.\n\n\n\n\n\n\n\n\nusing expert knowledge of and experience in data, statistical modeling, and machine learning methods; programming languages, packages/libraries, and big-data engineering solutions; manage and implement highly technical and complex data science projects from defining the vision to operationalizing solutions to ensure desired outcomes are achieved.\n\n\nprovides both technical and administrative supervision to the DSS team of four to five staff (on average) as we plan, develop, and implement data science projects, data engineering pipelines, and dashboards/visual analytics. The DSS team is comprised of two Data Scientists, a Jr. Data Scientist, a Data Engineer, and a Product Manager. We utilize the SCRUM framework to monitor and direct tasks, boards to communicate and evaluate progress, peer-to-peer code review in GitHub via pull request rules, and one on one sessions for pair programming and additional mentoring.\n\n\nwork closely with both senior Public Health Information Systems (PHIS) IT staff and Internal Services Department (ISD) IT staff on a variety of data infrastructure projects including the management and maintenance of our data science tooling (both the applications and the servers); work strategically on data engineering bottlenecks to enhance reliability and scalability of data; use knowledge of data science and data engineering practices to add suggestions for burgeoning information management and governance work; act as technical resource for team leads interested in leveraging DSS tooling.\n\n\nbuilding a robust R and Python community by collaborating closely with our RStudio partners to bring trainings to more than 180 staff within ACDC; helping to answer questions and support team members that run into issues related to version control, CI/CD, RStudio Connect, and other DSS tools; evangelize data modernization initiatives whenever possible while also staying up to date on the changing data science landscape.\n\n\nled DSS team to refactor (rewrite) from SAS and R codebases to large-scale (big data) architecture code that is automated with the Apache Airflow workflow tool leveraging Apache Spark via Python; these processes in some instances are three to four times faster than the original codebases, which we plan to continue to leverage for all of our Extract-Transfer-Load (ETL) needs.\n\n\ncontinuing to define and formalize the road map for data science within public health as an embedded group of expert resources that can consult and collaborate on a divisional or departmental opportunities/gaps in infrastructure or data engineering we can recommend plans for strengthening, streamlining, or augmenting.\n\n\n\n\n\n\n\n\nspearheading data integration projects with Whole Person Care program; optimizing links to internal master data management, countywide master data management, community/clinic based HIEs, and MCOs.\n\n\n\nhelp prototype operational dashboards for analyst team to put into production, working with clinical staff on creating tools that will add value rather than distract or oversaturate.\n\n\nwork with leadership on identifying KPIs and other operationally valuable markers for high level dashboard applications that allow users to explore and interact adding business value.\n\n\nliaise with external county departments, HIEs, and MCOs regarding data sharing standards and governance for WPC population–up to speed on current federal and state regulations for what can and what cannot be shared depending on context.\n\n\nassist in authoring analysis billing rules for data team to use to correctly produce mandatory reporting given complexities of WPC funding via Medi-Cal waiver program and shifting horizon of competing and complimentary programming.\n\n\nresponsible for calculating program milestones and incentives for state billing–working with front line program staff to improve workflows and capture the data the demonstrates achievement for the WPC program.\n\n\nact as bridge between IT, data team, evaluation team, and leadership on all data related matters–working to improve programming workflows through version control and software engineering best practices.\n\n\n\n\n\n\n\n\nprovide strategic oversight to organization’s data collection systems and reporting processes through the collection and analysis of data, reporting on and capturing of outcome metrics, and providing input to create a more data-driven culture\n\n\nresponsible for identifying, monitoring, and evaluating program and organizational outcomes to ensure compliance and effectiveness of programming\n\n\nmoved social service database from a no access closed system to AWS to reduce organizational cost and optimize IT ecosystem\n\n\nliaised with internal departments for reporting/proposal questions, and to answer questions about resident population trends, funding and compliance layers for affordable housing; and, how the two work toward a broader outcomes strategy\n\n\nprepare recurring and special aggregated data reports and related decision support tools for organization to support analytics value chain: datareportinganalysisactionvalue\n\n\noversaw the application and implementation of assessment procedures for longitudinal data collection; provided training in the use of survey instruments and answers to questions about what is tracked and why\n\n\nleveraged agglomerative clustering models, survival models, generalized linear models, and binary classifiers like logistic regression in order to build a wider understanding of how people fare in permanent supportive housing to see if we can detect negative outcomes early and target support services early.\n\n\nconverted all data workflows to version control via GitHub for all projects including web application and some of the data engineering\n\n\nliaise with external organizations, research institutions, and contractors / consultants interested in or currently conducting research with organization to answer questions about what we currently are using to track outcomes / offer support\n\n\n\n\n\n\n\n\nprovide in-depth analysis of both the resident population and underlying subsidy and compliance structure for entire portfolio\n\n\nleverage validated research tools to improve data collection; providing a pathway for future program evaluation based on real measurements that have reliable sensitivity and specificity\n\n\nexplored open-source database alternatives to reduce organizational cost and fit within the IT ecosystem\n\n\nactively participated in internal organizational dialog around the need to pivot from demonstrating need to demonstrating efficacy–championed the role data could play in program development, intervention, and outcomes\n\n\nprepared recurring and special aggregated data reports for asset management compliance, annual organizational audit, resident programs grant reporting, and housing development RFP/RSFQ submissions\n\n\n\nassist in any data related projects with community partners and research institutions\n\n\n\n\n\n\n\n\ngathered baseline data on current resident enrollment in Medi-Cal, their health care utilization and access, history of chronic health conditions, as well as food intake.\n\n\nperformed statistical analysis of health survey data using Python–we had 200 respondents and were able to make programmatic changes based on our findings\n\n\ndeveloped and implemented health training workshops and on-on-one guidance for Program Managers and Resident Services Coordinators to improve knowledge of new Affordable Care Act-driven insurance enrollment challenges for residents with advanced conditions\n\n\nimplemented resident-focused trainings on managing and seeking help for chronic health conditions as well as creating health literacy guides for residents\n\n\nplaned and coordinated resident health fairs in collaboration with community health providers to improve health literacy of residents in connection to both chronic conditions as well as the resources in their community\n\n\nprovided coordinating support to Program Managers and staff charged with oversight of existing on-site health resources and wellness programs. Explored potential partnerships to provide residents improved health and well-being\n\n\nprepared and maintained accurate reports and survey data utilizing the Efforts to Outcomes database\n\n\nproject troubleshooter par excellence.\n\n\n\n\n\n\n\n\nresponsible for administering and uploading survey data to the Substance Abuse and Mental Health Services Administration (SAMHSA) and its affiliates while maintaining confidentiality and records keeping due diligence\n\n\nworked with Resident Service Coordinators, programmatic staff, and grant evaluator on collecting and maintaining data pertinent mandated progress reports to SAMHSA\n\n\nresearched, wrote, and compiled bi-annual reports for the Cooperative Agreement to Benefit Homeless Individuals (CABHI) grant project\n\n\npoint-person/project manager for follow up and grant work-plan compliance with Skid Row Community Consortium\n\n\nresearched and identified future survey instruments for targeted sub-populations to improve data collection practices\n\n\nparticipated in regular trainings, program evaluation, and program development\n\n\ndesigned and edited Peer Advocate zine for commercial printing\n\n\nimproved client access to resources/information through small booklets tailored to the population\n\n\n\n\n\n\n\n\ndesigned and implemented new digital signage/print fliers to promote library outreach, services, and events\n\n\ncreated new social media presence for (then) new platform Vine to promote library collections and services\n\n\ndesigned online content that highlighted collections, incorporated teaching and learning services and provided wide audience appreciation\n\n\nfacilitated presentation on approach and ideas to UCLA Library’s social media steering committee\n\n\noperated multi-tiered social media platform apparatus\n\n\n\n\n\n\n\n\n\n\n\nResearched and implemented a suite of validated research questionnaires to collect survey data, deployed to database for end users to input; tidied, transformed, and modeled over 900 responses; and built and hosted a Shiny dashboard to communicate results. To be a part of the process from beginning to end was a huge learning process and I see it as a great accomplishment.\n\n\nDesigned, proofed, prepared, and labored over Peer Advocate ’Zine for Skid Row Housing Trust’s Hilton Project ’Zine “I Got You”, which will be released during a end of grant event for all project participants\n\n\nWas asked to help design and brainstorm a flier for the UCLA Library system because of interest in design and outreach for libraries–the final project was the “Top 10 Things” flier, which was a successful campaign to engage students in library programming and services\n\n\nWas recognized by two articles online for work promoting library services and collections through early-adoption of Vine\n\n\n\n\n\nLearning more about statistics and data science; particularly its applications in R. I am more and more becoming interested in Bayesian approaches.\n\n\nCoding: I will likely never code as neatly as an engineer but I try to incorporate software engineering best practices as I can–I am very keen to become more proficient with workflows in airflow, deploying APIs to AWS, and taking on more data engineering as practicable.\n\n\nDesign: I love typefaces and modernist design, in another life I would love to work as a designer or artist so it is something I passionately admire and sometimes try to do when time permits.\n\n\nExploration: I like to explore other places through travel and food; ideologies through reading and communicating, and world through science and discovery!\n\n\nUpdated on: r Sys.Date()"
  },
  {
    "objectID": "vitae copy.html#fa-compass-personal-philosophy",
    "href": "vitae copy.html#fa-compass-personal-philosophy",
    "title": "Full CV",
    "section": "",
    "text": "I believe that the 21st century requires a more multifaceted toolkit if we hope to solve complex challenges–process and task oriented approaches balanced within a framework that fosters strategic thinking. That interdisciplinary relationships can be strengthened through active community participation–working toward something with the shared value of doing good.\n I bring a lot of energy and enthusiasm to the projects that I become involved in. I thrive in situations that require imagination and innovation. My interests are varied; among my passions I would list: academic inquiry, classical and contemporary thought, the music and cuisine of other cultures, the arts, traveling, languages, and social justice. I believe that our most vulnerable deserve respect and to be treated with dignity and fairness.\n Ultimately I hope that the work I do is helpful–that it enables discovery or provides clarity. I hope to bring together practical and principled approaches and I hope to further grow as this philosophy guides me."
  },
  {
    "objectID": "vitae copy.html#fa-magic-personal-skills",
    "href": "vitae copy.html#fa-magic-personal-skills",
    "title": "Full CV",
    "section": "",
    "text": "I am dedicated and hard working–a good communicator and active community participant. I work well on team efforts and projects with the ultimate aim of using data for social good.\n I am:\n\n creative\n honest\n friendly\n enthusiastic\n keen to learn new things and take on new roles\n reliable\n a lateral thinker\n motivated\n\n I am able to:\n\n speak and read French\n read and understand some German\n listen to and follow instructions accurately\n work cooperatively\n take on new challenges\n complete tasks\n support others\n identify new innovations and technologies while working toward implementing them\n be proactive"
  },
  {
    "objectID": "vitae copy.html#fa-code-technical-skills",
    "href": "vitae copy.html#fa-code-technical-skills",
    "title": "Full CV",
    "section": "",
    "text": "Web Technologies: HTML, CSS, Javascript; Quarto publishing; Rmarkdown Websites, Dashboards in R with shiny/flexdashboard that support crosstalk, and interactive data visualization with Plotly; Static Site Generators, i.e., Pelican, Jekyll, and others; Traditional CMSs like Wordpress and Drupal; previous coursework in Information Architecture and UX/UI design.\n Coding: R, Python, and Javascript.\n OSs: macOS, Linux, Windows, and containers. Within the Linux ecosystem, I have experience with Debian based distributions as well as Red Hat Enterprise Linux. I love the terminal and wish vim bindings were universal.\n Data: Relational Databases (data modeling and SQL); Extract-Transfer-Load (ETL) and Extract-Load-Transfer (ELT) strategies for data engineering–specifically with Apache Airflow for ETL and Data Build Tool (dbt) for ELT; information retrieval; taxonomies and ontologies; and, knowledge organization theory.\n Data Science: both R and Python are a big part of my workflow from reading in data, transforming and ‘tidying’ data, exploring and visualizing data, modeling, reporting, presenting, and tying it all together in a dashboard. In connection to R, I am a proponent of the tidyverse framework for its unifying API. With respect to Python, I enjoy using polars, pyjanitor, pymc, and plotly to work in equivalent ways. I’m honestly happy working in both languages.\n Metadata schemas: knowledge of RDF, OWL, SPARQL, AACR2, RDA, MODS, METS, and PREMIS"
  },
  {
    "objectID": "vitae copy.html#fa-graduation-cap-education",
    "href": "vitae copy.html#fa-graduation-cap-education",
    "title": "Full CV",
    "section": "",
    "text": "University of North Texas | Master of Science in Information Science (2013) Information Systems track\n University of California, Los Angeles (UCLA) | Bachelor of Arts in French and comparative literature (2008 / double major)\n Université de la Sorbonne Nouvelle (Paris III) | Student exchange (2006–2007)"
  },
  {
    "objectID": "vitae copy.html#fa-institution-work-experience",
    "href": "vitae copy.html#fa-institution-work-experience",
    "title": "Full CV",
    "section": "",
    "text": "excited to bring my DevOps and Data Engineering skills to bear on thorny data problems in support of public good.\n\n\nSkylight is a digital government consultancy driven by making government work in a digital world through design, technology, and procurement.\n\n\n\n\n\n\n\n\nusing expert knowledge of and experience in data, statistical modeling, and machine learning methods; programming languages, packages/libraries, and big-data engineering solutions; manage and implement highly technical and complex data science projects from defining the vision to operationalizing solutions to ensure desired outcomes are achieved.\n\n\nprovides both technical and administrative supervision to the DSS team of four to five staff (on average) as we plan, develop, and implement data science projects, data engineering pipelines, and dashboards/visual analytics. The DSS team is comprised of two Data Scientists, a Jr. Data Scientist, a Data Engineer, and a Product Manager. We utilize the SCRUM framework to monitor and direct tasks, boards to communicate and evaluate progress, peer-to-peer code review in GitHub via pull request rules, and one on one sessions for pair programming and additional mentoring.\n\n\nwork closely with both senior Public Health Information Systems (PHIS) IT staff and Internal Services Department (ISD) IT staff on a variety of data infrastructure projects including the management and maintenance of our data science tooling (both the applications and the servers); work strategically on data engineering bottlenecks to enhance reliability and scalability of data; use knowledge of data science and data engineering practices to add suggestions for burgeoning information management and governance work; act as technical resource for team leads interested in leveraging DSS tooling.\n\n\nbuilding a robust R and Python community by collaborating closely with our RStudio partners to bring trainings to more than 180 staff within ACDC; helping to answer questions and support team members that run into issues related to version control, CI/CD, RStudio Connect, and other DSS tools; evangelize data modernization initiatives whenever possible while also staying up to date on the changing data science landscape.\n\n\nled DSS team to refactor (rewrite) from SAS and R codebases to large-scale (big data) architecture code that is automated with the Apache Airflow workflow tool leveraging Apache Spark via Python; these processes in some instances are three to four times faster than the original codebases, which we plan to continue to leverage for all of our Extract-Transfer-Load (ETL) needs.\n\n\ncontinuing to define and formalize the road map for data science within public health as an embedded group of expert resources that can consult and collaborate on a divisional or departmental opportunities/gaps in infrastructure or data engineering we can recommend plans for strengthening, streamlining, or augmenting.\n\n\n\n\n\n\n\n\nspearheading data integration projects with Whole Person Care program; optimizing links to internal master data management, countywide master data management, community/clinic based HIEs, and MCOs.\n\n\n\nhelp prototype operational dashboards for analyst team to put into production, working with clinical staff on creating tools that will add value rather than distract or oversaturate.\n\n\nwork with leadership on identifying KPIs and other operationally valuable markers for high level dashboard applications that allow users to explore and interact adding business value.\n\n\nliaise with external county departments, HIEs, and MCOs regarding data sharing standards and governance for WPC population–up to speed on current federal and state regulations for what can and what cannot be shared depending on context.\n\n\nassist in authoring analysis billing rules for data team to use to correctly produce mandatory reporting given complexities of WPC funding via Medi-Cal waiver program and shifting horizon of competing and complimentary programming.\n\n\nresponsible for calculating program milestones and incentives for state billing–working with front line program staff to improve workflows and capture the data the demonstrates achievement for the WPC program.\n\n\nact as bridge between IT, data team, evaluation team, and leadership on all data related matters–working to improve programming workflows through version control and software engineering best practices.\n\n\n\n\n\n\n\n\nprovide strategic oversight to organization’s data collection systems and reporting processes through the collection and analysis of data, reporting on and capturing of outcome metrics, and providing input to create a more data-driven culture\n\n\nresponsible for identifying, monitoring, and evaluating program and organizational outcomes to ensure compliance and effectiveness of programming\n\n\nmoved social service database from a no access closed system to AWS to reduce organizational cost and optimize IT ecosystem\n\n\nliaised with internal departments for reporting/proposal questions, and to answer questions about resident population trends, funding and compliance layers for affordable housing; and, how the two work toward a broader outcomes strategy\n\n\nprepare recurring and special aggregated data reports and related decision support tools for organization to support analytics value chain: datareportinganalysisactionvalue\n\n\noversaw the application and implementation of assessment procedures for longitudinal data collection; provided training in the use of survey instruments and answers to questions about what is tracked and why\n\n\nleveraged agglomerative clustering models, survival models, generalized linear models, and binary classifiers like logistic regression in order to build a wider understanding of how people fare in permanent supportive housing to see if we can detect negative outcomes early and target support services early.\n\n\nconverted all data workflows to version control via GitHub for all projects including web application and some of the data engineering\n\n\nliaise with external organizations, research institutions, and contractors / consultants interested in or currently conducting research with organization to answer questions about what we currently are using to track outcomes / offer support\n\n\n\n\n\n\n\n\nprovide in-depth analysis of both the resident population and underlying subsidy and compliance structure for entire portfolio\n\n\nleverage validated research tools to improve data collection; providing a pathway for future program evaluation based on real measurements that have reliable sensitivity and specificity\n\n\nexplored open-source database alternatives to reduce organizational cost and fit within the IT ecosystem\n\n\nactively participated in internal organizational dialog around the need to pivot from demonstrating need to demonstrating efficacy–championed the role data could play in program development, intervention, and outcomes\n\n\nprepared recurring and special aggregated data reports for asset management compliance, annual organizational audit, resident programs grant reporting, and housing development RFP/RSFQ submissions\n\n\n\nassist in any data related projects with community partners and research institutions\n\n\n\n\n\n\n\n\ngathered baseline data on current resident enrollment in Medi-Cal, their health care utilization and access, history of chronic health conditions, as well as food intake.\n\n\nperformed statistical analysis of health survey data using Python–we had 200 respondents and were able to make programmatic changes based on our findings\n\n\ndeveloped and implemented health training workshops and on-on-one guidance for Program Managers and Resident Services Coordinators to improve knowledge of new Affordable Care Act-driven insurance enrollment challenges for residents with advanced conditions\n\n\nimplemented resident-focused trainings on managing and seeking help for chronic health conditions as well as creating health literacy guides for residents\n\n\nplaned and coordinated resident health fairs in collaboration with community health providers to improve health literacy of residents in connection to both chronic conditions as well as the resources in their community\n\n\nprovided coordinating support to Program Managers and staff charged with oversight of existing on-site health resources and wellness programs. Explored potential partnerships to provide residents improved health and well-being\n\n\nprepared and maintained accurate reports and survey data utilizing the Efforts to Outcomes database\n\n\nproject troubleshooter par excellence.\n\n\n\n\n\n\n\n\nresponsible for administering and uploading survey data to the Substance Abuse and Mental Health Services Administration (SAMHSA) and its affiliates while maintaining confidentiality and records keeping due diligence\n\n\nworked with Resident Service Coordinators, programmatic staff, and grant evaluator on collecting and maintaining data pertinent mandated progress reports to SAMHSA\n\n\nresearched, wrote, and compiled bi-annual reports for the Cooperative Agreement to Benefit Homeless Individuals (CABHI) grant project\n\n\npoint-person/project manager for follow up and grant work-plan compliance with Skid Row Community Consortium\n\n\nresearched and identified future survey instruments for targeted sub-populations to improve data collection practices\n\n\nparticipated in regular trainings, program evaluation, and program development\n\n\ndesigned and edited Peer Advocate zine for commercial printing\n\n\nimproved client access to resources/information through small booklets tailored to the population\n\n\n\n\n\n\n\n\ndesigned and implemented new digital signage/print fliers to promote library outreach, services, and events\n\n\ncreated new social media presence for (then) new platform Vine to promote library collections and services\n\n\ndesigned online content that highlighted collections, incorporated teaching and learning services and provided wide audience appreciation\n\n\nfacilitated presentation on approach and ideas to UCLA Library’s social media steering committee\n\n\noperated multi-tiered social media platform apparatus"
  },
  {
    "objectID": "vitae copy.html#fa-trophy-accomplishments",
    "href": "vitae copy.html#fa-trophy-accomplishments",
    "title": "Full CV",
    "section": "",
    "text": "Researched and implemented a suite of validated research questionnaires to collect survey data, deployed to database for end users to input; tidied, transformed, and modeled over 900 responses; and built and hosted a Shiny dashboard to communicate results. To be a part of the process from beginning to end was a huge learning process and I see it as a great accomplishment.\n\n\nDesigned, proofed, prepared, and labored over Peer Advocate ’Zine for Skid Row Housing Trust’s Hilton Project ’Zine “I Got You”, which will be released during a end of grant event for all project participants\n\n\nWas asked to help design and brainstorm a flier for the UCLA Library system because of interest in design and outreach for libraries–the final project was the “Top 10 Things” flier, which was a successful campaign to engage students in library programming and services\n\n\nWas recognized by two articles online for work promoting library services and collections through early-adoption of Vine"
  },
  {
    "objectID": "vitae copy.html#fa-child-interests",
    "href": "vitae copy.html#fa-child-interests",
    "title": "Full CV",
    "section": "",
    "text": "Learning more about statistics and data science; particularly its applications in R. I am more and more becoming interested in Bayesian approaches.\n\n\nCoding: I will likely never code as neatly as an engineer but I try to incorporate software engineering best practices as I can–I am very keen to become more proficient with workflows in airflow, deploying APIs to AWS, and taking on more data engineering as practicable.\n\n\nDesign: I love typefaces and modernist design, in another life I would love to work as a designer or artist so it is something I passionately admire and sometimes try to do when time permits.\n\n\nExploration: I like to explore other places through travel and food; ideologies through reading and communicating, and world through science and discovery!\n\n\nUpdated on: r Sys.Date()"
  },
  {
    "objectID": "blog/llm/index.html",
    "href": "blog/llm/index.html",
    "title": "Your first chat bot with python!",
    "section": "",
    "text": "Goal\nLearn about language models and familiarize yourself with some of the basic functions of the {languagemodels} to create your own chat bot.\n{languagemodels} is a python module designed to be as simple as possible for learners and educators exploring how large language models intersect with modern software development. The interfaces to this package are all simple functions using standard types. The complexity of large language models is hidden from view while providing free local inference using light-weight, open models. All included models are free for educational use, no API keys are required, and all inference is performed locally by default.\nRead more about it\nYou can morph between plot like this:\n\nimport languagemodels as lm\nlm.do(\"What color is the sky?\")\n\n'The color of the sky is blue.'\n\n\nFetching 6 files:   0%|          | 0/6 [00:00&lt;?, ?it/s]\nFetching 6 files: 100%|##########| 6/6 [00:00&lt;00:00, 4316.61it/s]\n\n\n\nTo easy, let’s try something a bit harder\n\nlm.do(\"If I have 7 apples then eat 5, how many apples do I have?\")\n\n'You have 8 apples.'\n\n\nAouch…\nIndeed the model performance is quite low because the models used by this package are 1000x smaller than the largest models in use today. They are useful as learning tools, but if you are expecting ChatGPT or similar performance, you will be very disappointed…\nThe base model should work on any system with 512MB of memory, but this memory limit can be increased. Setting this value higher will require more memory and generate results more slowly, but the results should be superior. I let your try at home because this is rendered on a free server but if you uncomment lm.set_max_ram('4gb') you should get better results.\n\n#lm.set_max_ram('4gb')\nlm.do(\"If I have 7 apples then eat 5, how many apples do I have?\")\n\n'You have 8 apples.'\n\n\nYeah, here we go little (4gb) buddy!\n\nNow that we got the basics, let’s play with it!\n\nlm.chat('''\n     System: Respond as a helpful assistant.\n\n     User: What is relativity?\n\n     Assistant:\n     ''')\n\n'Relativity is the theory that explains how space and time are relative to each other.'\n\n\n\n\nlm.complete(\"She hid in her room until\")\n\n' she saw her dog.'\n\n\n\n\nlm.get_wiki(\"Physics\")\n\n'Physics is the natural science of matter, involving the study of matter, its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force. Physics is one of the most fundamental scientific disciplines, with its main goal being to understand how the universe behaves. A scientist who specializes in the field of physics is called a physicist.\\n\\nPhysics is one of the oldest academic disciplines and, through its inclusion of astronomy, perhaps the oldest. Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century these natural sciences emerged as unique research endeavors in their own right. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in these and other academic disciplines such as mathematics and philosophy.\\n\\nAdvances in physics often enable new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.'"
  },
  {
    "objectID": "blog/Data_Viz_Fundamentals/index.html",
    "href": "blog/Data_Viz_Fundamentals/index.html",
    "title": "Data Viz Fundamentals with R",
    "section": "",
    "text": "Goal\nGo over the basics of data visualization with R and learn more advanced concepts using the ggplot package.\nNote: All is in english\nOpen it full"
  },
  {
    "objectID": "blog/Live/index.html",
    "href": "blog/Live/index.html",
    "title": "We-Data Live on YouTube",
    "section": "",
    "text": "Live of December 29 edited in which we present in more detail WeData, its functioning and its future.\nNote: video in french, ask in comments for subtitle in your language\nVideo link"
  },
  {
    "objectID": "blog/Stats_With_R/index.html",
    "href": "blog/Stats_With_R/index.html",
    "title": "Satistics with R",
    "section": "",
    "text": "To cover the structure of a classical statistical analysis with R. We will then go into more depth on each part while presenting packages in the language that make the work easier. Although this playlist is designed for social science students, it can be useful for anyone.\nPlaylist here\nNote: video in french, ask in comments for subtitle in your language"
  },
  {
    "objectID": "blog/Stats_With_R/index.html#first-video-of-the-playlist",
    "href": "blog/Stats_With_R/index.html#first-video-of-the-playlist",
    "title": "Satistics with R",
    "section": "First video of the playlist:",
    "text": "First video of the playlist:"
  },
  {
    "objectID": "blog/Code_Hygiene/index.html",
    "href": "blog/Code_Hygiene/index.html",
    "title": "Code Hygiene. Don’t Laugh it off !",
    "section": "",
    "text": "By Fabrice Hategekimana\nGoal: Introduce the basics of hygiene in the code to have clean and reusable scripts. Workflow and refactoring are also important elements to achieve good results. We explain everything in our videos!\nPlaylist here\nNote: video in french, ask in comments for subtitle in your language"
  },
  {
    "objectID": "blog/Code_Hygiene/index.html#first-vidéo-of-the-playlist",
    "href": "blog/Code_Hygiene/index.html#first-vidéo-of-the-playlist",
    "title": "Code Hygiene. Don’t Laugh it off !",
    "section": "First vidéo of the playlist:",
    "text": "First vidéo of the playlist:"
  },
  {
    "objectID": "talks copy/2018-12-east-la-r-users/index.html",
    "href": "talks copy/2018-12-east-la-r-users/index.html",
    "title": "Interactive Dashboards with shiny",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "projects/2022-12-echarts4r/index.html",
    "href": "projects/2022-12-echarts4r/index.html",
    "title": "Echarts4r",
    "section": "",
    "text": "Full Screen\n\n\n\nDescription\n\n\n\nThis R package is a wrapper of the ECharts library. It provides a set of functions to generate interactive charts in R.  Highlights of this package:  - Provide functions to generate interactive charts in R. - The package is built on top of the ECharts library, which is a powerful library for data visualization. - The package provides a set of functions that can be used to generate charts with ECharts. - The package is still under development. More features will be added in the future. - The package is open source.   Features of the package: \n\nProvides more than 20 chart types available out of the box, along with a dozen components, and each of them can be arbitrarily combined to use.\nHas a powerful rendering engine that allows you to easily switch between Canvas and SVG rendering. Progressive rendering and stream loading make it possible to render 10 million data in realtime.\nOffers professional data analysis through datasets, which support data transforms like filtering, clustering, and regression to help analyze multi-dimensional analysis of the same data.\nHas an elegant visual design that follows visualization principles and supports responsive design. Flexible configurations make it easy to customize.\nHas a healthy community that ensures the healthy development of the project and contributes a wealth of third-party extensions.\nIs accessibility-friendly with automatically generated chart descriptions and decal patterns that help users with disabilities understand the content and the stories behind the charts.\n\n\nYou can check the project on Github."
  },
  {
    "objectID": "blog/llm/llm.html",
    "href": "blog/llm/llm.html",
    "title": "Your first chat bot with python!",
    "section": "",
    "text": "By David Munoz Tord\nGoal: Learn about language models and familiarize yourself with some of the basic functions of the {languagemodels} to create your own chat bot.\n{languagemodels} is a python module designed to be as simple as possible for learners and educators exploring how large language models intersect with modern software development. The interfaces to this package are all simple functions using standard types. The complexity of large language models is hidden from view while providing free local inference using light-weight, open models. All included models are free for educational use, no API keys are required, and all inference is performed locally by default.\nRead more about it\nYou can morph between plot like this:\n\nimport languagemodels as lm\nlm.do(\"What color is the sky?\")\n\n\nTo easy, let’s try something a bit harder\n\nlm.do(\"If I have 7 apples then eat 5, how many apples do I have?\")\n\nAouch…\nIndeed the model performance is quite low because the models used by this package are 1000x smaller than the largest models in use today. They are useful as learning tools, but if you are expecting ChatGPT or similar performance, you will be very disappointed…\nThe base model should work on any system with 512MB of memory, but this memory limit can be increased. Setting this value higher will require more memory and generate results more slowly, but the results should be superior. Let’s try:\n\nlm.set_max_ram('4gb')\nlm.do(\"If I have 7 apples then eat 5, how many apples do I have?\")\n\nYeah, here we go little (4gb) buddy!\n\nNow that we got the basics, let’s play with it!\n\nlm.chat('''\n     System: Respond as a helpful assistant.\n\n     User: What is relativity?\n\n     Assistant:\n     ''')\n\n\n\nlm.complete(\"She hid in her room until\")\n\n\n\nlm.get_wiki(\"Physics\")"
  },
  {
    "objectID": "index.html#hi-im-david.-im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project-to-work-on",
    "href": "index.html#hi-im-david.-im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project-to-work-on",
    "title": "munoztd0",
    "section": " Hi, I’m David. I’m a Data Scientist / R Developer always looking for a cool open-source project to work on !",
    "text": "Hi, I’m David. I’m a Data Scientist / R Developer always looking for a cool open-source project to work on !"
  },
  {
    "objectID": "index.html#hi-im-david.",
    "href": "index.html#hi-im-david.",
    "title": "munoztd0",
    "section": " Hi, I’m David. ",
    "text": "Hi, I’m David. \nI’m a Data Scientist / R Developer always looking for a cool open-source project !"
  },
  {
    "objectID": "index.html#im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project-to-work-on",
    "href": "index.html#im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project-to-work-on",
    "title": "munoztd0",
    "section": "I’m a Data Scientist / R Developer always looking for a cool open-source project to work on !",
    "text": "I’m a Data Scientist / R Developer always looking for a cool open-source project to work on !"
  },
  {
    "objectID": "index.html#im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project",
    "href": "index.html#im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project",
    "title": "munoztd0",
    "section": "I’m a Data Scientist / R Developer always looking for a cool open-source project !",
    "text": "I’m a Data Scientist / R Developer always looking for a cool open-source project !"
  },
  {
    "objectID": "index.html#hi-im-david.-im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project",
    "href": "index.html#hi-im-david.-im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project",
    "title": "David Munoz Tord",
    "section": " Hi, I’m David.   I’m a Data Scientist / R Developer always looking for a cool open-source project ! ",
    "text": "Hi, I’m David.   I’m a Data Scientist / R Developer always looking for a cool open-source project !"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "We Data",
    "section": "",
    "text": "About us\n\nWe are an association of the University of Geneva that aims to share its passion for data science and computer science. We have a strong interest in statistics and computational methods. Initially, the association’s target audience was people in the social sciences but we quickly expanded into other fields of research.  More concretely, the association aims to achieve its objectives by creating freely-accessible educational content on its platforms in a variety of forms: YouTube videos, blog posts, exercises, etc. The association co-organizes the R-Lunches, a series of events related to R in which various speakers present topics related to the R language. The association tries to keep abreast of and participate as often as possible in digital initiatives at the University of Geneva. \n\n\n\nStill curious? click ‘About’ above or this button here: Tell me more\n\n\n\n\n\n\n\nBlog\nLatest from the blog\n\n\n\n\nR and Python side-by-side for data wrangling\n\n Feb 11, 2024\n\nNo matching items\n\n\n  \n\n\n\n\nBlog\n\n\n\n\nTalks\nLatest talk I’ve done\n\n\n\n\n\nTalks\n\n\n\n\nProjects\nLatest tinkering\n\n\n\n\n\nProjects\n\n\n\n\n\nTalking about us\nVie de l'UNIGE Des vidéos pour acquérir des compétences en science des données  September 2023\nFull Article"
  },
  {
    "objectID": "index.html#hi",
    "href": "index.html#hi",
    "title": "David Munoz Tord",
    "section": " Hi  ",
    "text": "Hi"
  },
  {
    "objectID": "blog/True_Ignorant/index.html",
    "href": "blog/True_Ignorant/index.html",
    "title": "What a tool! - An ignorant perspective",
    "section": "",
    "text": "By Fabrice Hategekimana\nIn this playlist are all the videos presenting useful tools when coding or when you are a student.\nPlaylist here\nNote: video in french, ask in comments for subtitle in your language"
  },
  {
    "objectID": "blog/True_Ignorant/index.html#first-video-of-the-playlist",
    "href": "blog/True_Ignorant/index.html#first-video-of-the-playlist",
    "title": "What a tool! - An ignorant perspective",
    "section": "First video of the playlist:",
    "text": "First video of the playlist:"
  },
  {
    "objectID": "blog/Echarts4r/index.html",
    "href": "blog/Echarts4r/index.html",
    "title": "Data Viz with Echarts4r",
    "section": "",
    "text": "Goal\nLearn about data visualization and familiarize yourself with some of the basic functions of the {echarts4r}.\n{echarts4r} is back! And with version 4.5 the new features from version 5 of echarts.js are available now. Moreover, the morphing capabilities of echart.js have been ported to echarts4r as we will show in this post.\nRead more about it\nYou can morph between plot like this:\n\n\nlibrary(echarts4r)\n\nmtcars2 &lt;- mtcars |&gt; \n  head() |&gt; \n  tibble::rownames_to_column(\"model\")\n\ne1 &lt;- mtcars2 |&gt; \n  e_charts(model) |&gt; \n  e_bar(\n    carb, \n    universalTransition = TRUE,\n    animationDurationUpdate = 1000L\n  )\n\ne2 &lt;- mtcars2 |&gt; \n  e_charts(model) |&gt; \n  e_pie(\n    carb, \n    universalTransition = TRUE,\n    animationDurationUpdate = 1000L\n  )\n\ncb &lt;- \"() =&gt; {\n  let x = 0;\n  setInterval(() =&gt; {\n    x++\n    chart.setOption(opts[x % 2], true);\n  }, 3000);\n}\"\n\n\n\n\ne_morph(e1, e2, callback = cb)\n## Warning in e_morph(e1, e2, callback = cb): This is experimental"
  },
  {
    "objectID": "blog/2023-07-16-contributing-to-project-on-github/index.html#start-small-aim-big",
    "href": "blog/2023-07-16-contributing-to-project-on-github/index.html#start-small-aim-big",
    "title": "Contributing to projects on GitHub",
    "section": "Start Small, Aim Big",
    "text": "Start Small, Aim Big\nFirst things first, let’s get this out of the way. There is no such thing as a ‘too small’ contribution. Even a single-line change, fixing a typo, or enhancing the readability of a README file – these seemingly minuscule contributions already have an impact in open-source projects."
  },
  {
    "objectID": "blog/2023-07-16-contributing-to-project-on-github/index.html#the-perks-of-contributing",
    "href": "blog/2023-07-16-contributing-to-project-on-github/index.html#the-perks-of-contributing",
    "title": "Contributing to projects on GitHub",
    "section": "The Perks of Contributing",
    "text": "The Perks of Contributing\n“But why should I contribute?”, you might ask. Excellent question! Here’s why:\n\nPersonal Growth: Contributing to open-source projects can improve your skills, give you practical experience, expand your network, and even potentially catch the eyes of potential employers or collaborators. Your GitHub account is a portfolio that show what your are doing, it is the solid proof of your competences.\nImproving the Software: By contributing, you help in maintaining and enhancing the project, ensuring its longevity and reliability. That’s why we recommand you to chose projets that you like or could bring you valuable outcome in your coding learning.\nTake and Give: Contributing is a fantastic way to give back to the community that has probably helped you in one way or another."
  },
  {
    "objectID": "blog/2023-07-16-contributing-to-project-on-github/index.html#your-first-contribution",
    "href": "blog/2023-07-16-contributing-to-project-on-github/index.html#your-first-contribution",
    "title": "Contributing to projects on GitHub",
    "section": "Your First Contribution",
    "text": "Your First Contribution\nFor the rest of the tutorial you need to have a GitHub account and to understand how it works. If you don’t know there are, here are some tutorials:\n\nEnglish\nFrench\nGerman\n\nIf you have the basic of Git and GitHub, let’s get started. Here’s the nitty-gritty:\n\nChoose your Quest: Find a project that piques your interest. It could be anything, from a complex machine learning library to a simple script that automates meme generation (because who doesn’t love memes?). Keep in mind to chose projects that goes into your interest and that can bring you joy and wisdom. Even if the project seems chalenging and you think you don’t have the level, just ask the owner of the project what you can bring to it at your level. Even doing the documentation is a good way to start (See bellow in ==complet here==). Sometimes owner of project write themselve what are the things to take care of and openly ask for help in specific area (or in the whole project).\nFork it: Fork the repository to your account. Think of it as creating your own magic clone of the project where you can tinker without disturbing the original.\nClone it: Clone the forked repository onto your local machine. Basically, it’s like summoning your cloned project from the cloud onto your computer.\nCreate a Branch: It’s always a good practice to create a new branch for your changes. Consider this as creating a parallel universe where your changes won’t affect the main storyline.\nMake Changes: Here comes the fun part. Dive in and make your changes. Remember, no contribution is too small.\nCommit and Push: Once you’re satisfied with your changes, commit them with a clear, informative message and push them to your forked repository.\nPull Request (PR): Back on the GitHub site, you can now open a PR against the original repository. It is sending your change to the main project online. Then the owner of the project can read your changes and see if they can accept it.\nWait: Now, the repository owner reviews your changes. If they like them, they’ll merge them into the project. Congrats, you just made your first contribution!\nCelebrate: Grab yourself a cake, you’ve earned it!"
  },
  {
    "objectID": "blog/2023-07-16-contributing-to-project-on-github/index.html#quick-tricks-for-a-rapid-start",
    "href": "blog/2023-07-16-contributing-to-project-on-github/index.html#quick-tricks-for-a-rapid-start",
    "title": "Contributing to projects on GitHub",
    "section": "Quick Tricks for a Rapid Start",
    "text": "Quick Tricks for a Rapid Start\nAlright, here are some tricks for a fast start:\n\n‘Good First Issues’: Many repositories tag some issues as Good First Issues or ‘Beginner-Friendly’. These are perfect for getting your feet wet. You can then go to GitHub and look for those terms in the search bar.\nRead the Docs: Always, ALWAYS, read the project’s README and CONTRIBUTING guide before starting. Every repository is a new world with its own set of rules.\nCommunicate: Open-source is all about collaboration. If you’re unsure about something, just ask. The community is generally friendly and helpful.\nDon’t Fear Rejection: Sometimes your changes might not get accepted. That’s okay! Each rejection is a stepping stone to improvement. Learn from it, and keep contributing.\n\nIn the end, contributing to GitHub is not just about writing code, it’s about being a part of something bigger than yourself. So, are you ready to contribute your skills of GitHub?"
  },
  {
    "objectID": "blog/2023-07-16-contributing-to-project-on-github/index.html#the-r-case-contributing-to-r-packages",
    "href": "blog/2023-07-16-contributing-to-project-on-github/index.html#the-r-case-contributing-to-r-packages",
    "title": "Contributing to projects on GitHub",
    "section": "The R case: Contributing to R Packages",
    "text": "The R case: Contributing to R Packages\nIf you’re familiar with the R’s universe, contributing to R packages can be a rewarding adventure. Why? Because in this realm, the structure is your friend. You’ll find it’s not just facile but rather enjoyable!\n\nWhy Contribute to R Packages?\nR packages are the lifeblood of the R ecosystem, facilitating data analysis, visualization, and much more. When you contribute to R packages, you are helping to enhance the functionality, efficiency, and reliability of these packages.\n\n\nHow to Contribute to R Packages?\nThe best ressource to learn how to create solid packages in R is the following book freely available online: R packages\nHere are some interesting steps to guide you through the process:\n\nIdentify the Package: First, identify the R package you want to contribute to. It could be a R package you use often or one you believe could benefit from your magical touch. For instance you could have noticed specific error or way to improve it in your taste that could please others.\nCheck the Package Guidelines: Each R package usually has its own contribution guidelines. Take a moment to read these, as they often contain valuable information on the package’s style and testing procedures. Remember, when in Rome, do as the Romans do.\nSetup Your R Environment: You will need an integrated development environment (IDE) for R. RStudio is a fantastic choice (VS Code is also a good choice). Next, install the devtools package. This package provide functions to simplify package development and maintenance.\nTo install devtools, you can use:\ninstall.packages(\"devtools\")\nFork and Clone the Package: Similar to the steps described earlier, fork the repository of the package to your GitHub account, and then clone it onto your local machine.\nLoad the Package: To load the package in R, use the load_all() function from the devtools package. This function loads all the functions and data in the package into your R environment, which lets you test your changes quickly and easily.\ndevtools::load_all()\nBranch Out: As with other contributions, it’s wise to create a new branch for your changes.\nMake Changes: Sprinkle your magic dust. You can add features, fix bugs, improve documentation, or even enhance performance.\nTest Your Changes: Testing is crucial. Use the test() function from the devtools package to run the package’s test cases and ensure your changes don’t break anything.\ndevtools::test()\nDocument Your Changes: Use the document() function from devtools to update the package documentation.\ndevtools::document()\nCommit, Push, and PR: Once satisfied with your changes, commit them with a meaningful message, push to your forked repository, and open a PR against the original repository.\n\nR package development has its charms and challenges, but the joy of contributing and improving the package for the entire R community is undoubtedly enchanting."
  },
  {
    "objectID": "blog/Two_minutes_rule/index.html",
    "href": "blog/Two_minutes_rule/index.html",
    "title": "The two-minute rule for busy coders/learners",
    "section": "",
    "text": "For busy readers\n\n\n\n\nWe generally have time on our hands, but what makes us feel short of time is the prioritization of our activities.\nStarting to code is difficult, so the activity will be low on our priority list.\nTo make it easier to get started, you need to start with 2-5 minute sessions every week and gradually increase the importance of the activity.\nTo find out how to put this into practice and discover other tips, read the blog post.\n\n\n\nHey, aspiring coders! Let’s dive deeper into making coding a seamless part of your busy life. If you’re struggling to find time, the two-minute rule is your new best friend. It’s all about setting yourself up for success with quick, easy-to-start tasks. Let’s expand on this and give you some real actionable steps!\n\n\n\n\n\n\nNote\n\n\n\nAs you read this article, please take note so that you do not need to come back too often. And be sure to apply everything right away!😁\n\n\n\n\nThe Two-Minute Rule is pretty straightforward: If a habit is hard to get into because of a lack of time, try to start slowly with short 2-minute sessions. This principle, a fusion of the Atomic habits philosophy and the Kaizen method, is perfect for building a coding habit. It’s about overcoming the inertia of starting by making the first step super easy. It is a practice that emphasizes continuous improvement through small, incremental changes in habits while getting rid of bad habits (which is not the focus today).\nIn concrete terms, you start by forging a habit by being regular for a small period of time, in our case 2 minutes every week. Then, when the habit is acquired, you can start to increase the duration and frequency according to what suits you best. You can start with 5 or 10 minutes if that’s not too much of a challenge.But don’t be greedy and hasty at first!\nIndeed we are looking for long term results. Of course, nothing restrain you to have non-regular coding period outside of your new habit, but keep the regularity with this specific habit!\nHow to apply that in your coding habit? Follow this steps:\n\nSet clear objectives: Making a roadmap is event better! There are lots of different things to learn in coding. Not everything is necessary, and that’s why it’s important to set concrete, practical objectives. For example:\n\nFor Python it might be “Know how to create a complete machine learning workflow” rather than “Know how to do machine learning”.\nFor R it might be “Apply a Bayesian multilevel model in a research project on wage inequality” rather than “Know how to use Bayesian statistics”.\n\nDivide this/these objective(s) in multiple smaller steps: To avoid being overwhelmed by a huge goal, you need to break it down into smaller, chronological and measurable objectives. If you’re having trouble doing this, there are plenty of roadmap infographics on the Internet, and in the worst case, ask Chat-GPT or another model to do it for you. Going back to the previous examples:\n\nFor Python it could be:\n\n“Import data”\n“Clean data”\n…\n“Apply random forest model”\n“Compare performance of different models”\n“Make predictions”\n“Export model”…\n\nFor R it could be:\n\n“Select appropriate data”\n“Collect data”\n…\n“Perform descriptive analysis”\n“Specify appropriate priors”\n“Optimize the model”\n“Create regression tables”…\n\n\nSet a precise time: To anchor a good habit, it’s important to rely on regularity rather than effort. Start by setting a simple rhythm (in this case, 2 minutes) and, above all, a precise context. For example: “Every Friday at 6 p.m. on my table in my room, I’m going to learn to code, for 2 minutes”. You can increase the length and frequency of these sessions as you get used to them. There’s no point in starting intensively - it’s the regularity that counts.\nIdentify obstacles and set up a coding zone: See the next section for more details!\n\n\n\n\nThink about what stops you from coding. Here are some classical examples:\n\nDistraction in your PC/smartphone?\nIs it setting up your environment?\nFinding the right resources?\nLack of roadmap or precise steps?\nStarting take too much time (more than 2 minutes)?\nForgetting everything each time?\nNon regularity?\n\nWrite these down and tackle them!\nThe best way to resolve most of the problems is to create an environment that invites you to code and get started fast. Here’s how:\n\nEliminate distractions: Turn off non-essential notifications and put your smartphone aside. You have little time to focus on your work, don’t waste it!\nDedicated space: Even if it’s just a corner of your desk, make it your “coding zone.”\n\nProjects: On your computer, use projects which are dedicated environement (generally a folder) in which you find all the scripts and tools you need to start coding. For instance, Rstudio and VS code have their dedicated way to organise project.\nKeep things tidy: Spend time organizing your projects and your code in a clean way. Every minute you spend getting your project clean will be hours of work you’ll save yourself in the future. What’s more, your project will become a great example for your future works, and therefore a golden resource!\n\nKeep your tools handy: Have your laptop, charger, and any books or resources you need within reach.\n\nBookmark resources: Keep tabs open with coding websites or tutorials you’re following to check anything you need. Ai tools are also ressources, but limit their use (you risk becoming dependent and learning nothing).\nTake notes as you learn: The best ressources are the one you make. Keep a file open (Markdown, Word document, etc.) to note everything useful to help you remember tips/tricks and to write down steps to get started fast (you rarely can remember everything the first time).\nLook for productivity tools: Many code editor like RStudio or VS Code have powerfull extensions. I highly recommand TODO extensions on both RStudio and VS Code, since they let you start right away with specific tasks on your code and help you navigate them.\n\nLook for accountability: It’s no easy task to forge a new habit and stick to it. Often it’s events beyond our control that prevent us from sticking to them. That’s why it’s important to think ahead, to make it difficult to get out of these habits and easy to get in. The most important thing is not to blame ourselves, but to get back into the swing of things as soon as possible:\n\nPlan B: Not only do you need to choose a good time that won’t be disrupted, but you also need to anticipate how you can make up for any unforeseen circumstances. You need to plan ahead for a Plan B, which is another time when the session can be rescheduled. It’s not enough to “make up the time” at the next session; what counts is regularity, not time spent.\nPlan C: There are extreme cases where it’s literally impossible to continue the activity as desired. In such cases, coding is no longer possible (prolonged absence, lost computer, broken hand, etc.). If possible, you need to plan ahead for a viable alternative to keep up the pace. This doesn’t necessarily mean coding. The following section gives a list of things you can do.\nWorking with colleagues: The best way to stay accountable without putting too much blame or pressure on yourself is to join a work group that has set itself a goal of regularity (it doesn’t have to be the same subject). For example, you can organize regular face-to-face or online meetings. During these meetings, everyone works on their personal project. Or you can simply share your progress without the need for meetings. Whatever suits you!\n\n\nAll of the above may seem like big challenges, especially if you’re not familiar with them. But don’t worry, you don’t have to apply them all at once. I’ve arranged them in order of importance. Concentrate on one element at a time until it becomes natural for you. Then you can concentrate on the next element. Since eliminating distractions and finding a dedicated place to work are required, you can start with setting up your project, then move on to keeping your work area clean, then move on to making resources available, and so on. See? A piece of cake!\n\n\n\nOk, we have everyting to start. So what now? What can you possibly do in two, five our ten minutes? Also what about the extrem cases when we can’t possibly code? Let’s break down some coding activities:\n\nNote: activities that can be done whith a smartphone are marked with a (📱) and activities tha can be done without coding are marked with a (✅). Also, when no technology is needed it is marked with a (✏️).\n\n\nTwo- or five-minute tasks:\n\nDo an online coding exercise (here is a list of good platforms) 📱\nRead a tutorial/documentation/book and take note 📱✅✏️\nDrill a simple script to memorize it\nSet up a working directory in the computer✅\nOrganise a project in smaller steps✅\nWrite a comment/TODO in the code explaining your next steps.\nRead a code snippet and type it out.\nBookmark a tutorial for your next session. 📱✅\nSet new goals✅✏️\n\nTen- or five-minute tasks:\n\nAll the previous tasks\nStart a new project (divided in multiple sessions)\nTry a new library/package\nDebug code or find solutions on internet\nFollow a short YouTube tutorial (it is better to code in the same time) 📱✅\nOrganise/clean the project ✅\n\nThirty-Minute or more tasks:\n\nAll the previous tasks\nBuild a small project\nWork through longer tutorials (you should definitely code along)\nLearn new coding tricks 📱\nLearn other coding tools (GitHub for instance)\nLearn new languages (latex, regex, html, etc.)\n\nWhen coding is impossible:\n\nOrganize your roadmap 📱✅✏️\nDiscuss about programming 📱✅✏️\nCompile new resources 📱✅✏️\n\n\n\n\n\nWe’ve finished the main part of the tutorial, so you can stop here. If you’re still not sure how to get started, in the next section we present a concrete example.\n\n\n\n\n\n\nImportant\n\n\n\nBefore you move on, choose a two-minute task from this list and do it. Right now (or plan it for later). It’s all about building momentum!\n\n\n\n\n\nNow we have everything we need to start these new habits. Let’s take the example of Sandiya, a geographer who wants to learn how to use R for a future article. The problem is that she knows nothing about R. So she decides to use the two-minute principle.\n\n\nOnce upon a time in the bustling city, there was a geographer named Sandiya. Passionate about urbanism, she had a dream: to create a detailed map of urban car density using geodata from parking places. But there was a catch – she didn’t know R, the programming language perfect for this task.\nStep 1: Defining the goal\nSandiya’s project was clear: analyze car density variability in various parking places over time. But to do this, she needed to learn R, master data analysis, understand web scraping, and get a grip on geospatial analysis.\nStep 2: Crafting a roadmap\nAfter fruitful discussions and online research, Sandiya outlined her learning path:\n\nBasics of R\nData Analysis Skills\nWeb Scraping Knowledge\nGeospatial Analysis with R\n\nWith her roadmap in hand, she decided to start small, applying the Two-Minute Rule.\nStep 3: Setting the habit\nEvery Tuesday at 4 pm, Sandiya’s living room transformed into her learning zone. She started with just two minutes of R practice and gradually increased her time.\nStep 4: The initial strides\nIn her first session, Sandiya installed R and RStudio, a small but crucial step. The following week, she began online exercises on W3Schools, steadily progressing through them. It was a wise choice, since the website is completly free, doesn’t require an account and has tutorials on R.\nStep 5: Expanding the learning schedule\nTwo months in, Sandiya was already diving into “R for Data Science” (a free online book to learn R). She added Thursdays at 4 pm to her schedule, now feeling more confident and eager to learn. She was already working 30 minutes per session.\nStep 6: Gaining confidence\nAfter another two months, she had honed her data analysis skills. Tackling a data analysis tutorial, she replicated the examples, cementing her understanding.\nStep 7: Diving into web scraping\nSandiya then turned to YouTube videos and blog posts to learn web scraping. She meticulously took notes, starting to collect her needed data. She needed more time, so she decided to work at least one hour per session.\nStep 8: Embracing geospatial analysis\nSimultaneously, she delved into a Bookdown on geospatial analysis in R, replicating examples and building her skills.\nStep 9: Six months of transformation\nSix months from her humble two-minute beginnings, Sandiya was working six hours weekly on R (three hours each on Tuesday and Thursday) and had reached an advanced level. She was able to submit her paper for publication.\nStep 10: Flexibility and persistence\nNot every week was perfect. Sometimes, life intervened, and Sandiya rescheduled her Tuesday sessions to Saturday mornings. But she persisted.\nStep 11: Eyeing a new challenge\nNow, comfortably adept at R and with a regular habit firmly in place, Sandiya began contemplating learning Python, using the same effective, gradual method.\nSandiya’s story is a testament to the power of small, consistent steps. Starting with just two minutes of focused time, she transformed her skill set and realized her dream. Her journey, marked by clear goals, a structured roadmap, and adaptable persistence, is an inspiring blueprint for anyone looking to embark on a similar path. Remember, every big accomplishment starts with one small step. Just like Sandiya, you too can achieve your learning goals, one small, focused session at a time. 🌟💻🗺️\n\nRemember, coding doesn’t have to be a daunting time-sink. With the Two-Minute Rule, you’re not just learning to code; you’re integrating it into your life in manageable, enjoyable pieces. Keep it up, and watch how these minutes add up to significant progress. Happy coding, friends! 🚀💻✨"
  },
  {
    "objectID": "blog/Two_minutes_rule/index.html#final-example",
    "href": "blog/Two_minutes_rule/index.html#final-example",
    "title": "The two-minute rule for busy coders/learners",
    "section": "",
    "text": "Now we have everything we need to start these new habits. Let’s take the example of Sandiya, a geographer who wants to learn how to use R for a future article. The problem is that she knows nothing about R. So she decides to use the two-minute principle.\n\n\nOnce upon a time in the bustling city, there was a geographer named Sandiya. Passionate about urbanism, she had a dream: to create a detailed map of urban car density using geodata from parking places. But there was a catch – she didn’t know R, the programming language perfect for this task.\nStep 1: Defining the goal\nSandiya’s project was clear: analyze car density variability in various parking places over time. But to do this, she needed to learn R, master data analysis, understand web scraping, and get a grip on geospatial analysis.\nStep 2: Crafting a roadmap\nAfter fruitful discussions and online research, Sandiya outlined her learning path:\n\nBasics of R\nData Analysis Skills\nWeb Scraping Knowledge\nGeospatial Analysis with R\n\nWith her roadmap in hand, she decided to start small, applying the Two-Minute Rule.\nStep 3: Setting the habit\nEvery Tuesday at 4 pm, Sandiya’s living room transformed into her learning zone. She started with just two minutes of R practice and gradually increased her time.\nStep 4: The initial strides\nIn her first session, Sandiya installed R and RStudio, a small but crucial step. The following week, she began online exercises on W3Schools, steadily progressing through them. It was a wise choice, since the website is completly free, doesn’t require an account and has tutorials on R.\nStep 5: Expanding the learning schedule\nTwo months in, Sandiya was already diving into “R for Data Science” (a free online book to learn R). She added Thursdays at 4 pm to her schedule, now feeling more confident and eager to learn. She was already working 30 minutes per session.\nStep 6: Gaining confidence\nAfter another two months, she had honed her data analysis skills. Tackling a data analysis tutorial, she replicated the examples, cementing her understanding.\nStep 7: Diving into web scraping\nSandiya then turned to YouTube videos and blog posts to learn web scraping. She meticulously took notes, starting to collect her needed data. She needed more time, so she decided to work at least one hour per session.\nStep 8: Embracing geospatial analysis\nSimultaneously, she delved into a Bookdown on geospatial analysis in R, replicating examples and building her skills.\nStep 9: Six months of transformation\nSix months from her humble two-minute beginnings, Sandiya was working six hours weekly on R (three hours each on Tuesday and Thursday) and had reached an advanced level. She was able to submit her paper for publication.\nStep 10: Flexibility and persistence\nNot every week was perfect. Sometimes, life intervened, and Sandiya rescheduled her Tuesday sessions to Saturday mornings. But she persisted.\nStep 11: Eyeing a new challenge\nNow, comfortably adept at R and with a regular habit firmly in place, Sandiya began contemplating learning Python, using the same effective, gradual method.\nSandiya’s story is a testament to the power of small, consistent steps. Starting with just two minutes of focused time, she transformed her skill set and realized her dream. Her journey, marked by clear goals, a structured roadmap, and adaptable persistence, is an inspiring blueprint for anyone looking to embark on a similar path. Remember, every big accomplishment starts with one small step. Just like Sandiya, you too can achieve your learning goals, one small, focused session at a time. 🌟💻🗺️\n\nRemember, coding doesn’t have to be a daunting time-sink. With the Two-Minute Rule, you’re not just learning to code; you’re integrating it into your life in manageable, enjoyable pieces. Keep it up, and watch how these minutes add up to significant progress. Happy coding, friends! 🚀💻✨"
  },
  {
    "objectID": "blog/Coding_road_map/Index.html",
    "href": "blog/Coding_road_map/Index.html",
    "title": "A Computer Science Roadmap",
    "section": "",
    "text": "A Computer Science Roadmap: From Theory to Mastery\nHey there, future coding maestro! Venturing into the vast realm of computer science can seem intimidating. Fear not! This roadmap will guide you through theoretical foundations, essential languages, and core software engineering practices, all while encouraging you to explore further.\nNote: It’s clearly an exaggeration to sum up all computer science learning in a single post. In reality, each part deserves its own roadmap. But for the moment we’re content with this because of the structure of our channel. When we get more content on our YouTube channel, we’ll be making a lot of changes to this roadmap. So, don’t see this content as the entirety of what you need to learn, but simply as the beginning of a journey under construction!\n\n\n1. Theoretical Foundations\nBefore diving into coding, understanding the basics will serve you well throughout your journey.\n\nAlgorithms & Data Structures: Learn how algorithms work and why certain data structures are preferred for specific tasks.\n\n\n\n\n2. Introduction to Shell Programming\nThe command-line interface (CLI) is a powerful tool.\nBasic\n\nBasics of CLI: Navigate directories, manage files, and get comfortable with the terminal.\n\n\nTerminal playlist\n\nShell Scripting: Automate tasks using bash (or your shell of choice) scripting.\n\n\n\nModern Shell: Let’s discover some modern shell and their interesting features. Here we dive into Nushell!\n\n\nNushell playlist\n\n\n\n3. Python\nA versatile language with a gentle learning curve.\n\nPython Basics: Variables, loops, conditions, and functions.\n\n\nPython’s fundamental playlist\n\nAdvanced Python: Object-oriented programming, list comprehensions, and modules.\nPython Ecosystem: Libraries like numpy, pandas, and flask to supercharge your projects.\n\n\n\n\n4. Rust\nA language focusing on performance and safety.\n\nRust Fundamentals: Understand ownership, borrowing, and lifetimes.\n\n\nRust playlist\n\nConcurrency in Rust: Explore Rust’s approach to threads and safe concurrent programming.\n\n\n\n\n5. Haskell\nDive into the world of functional programming.\n\nHaskell Basics: Grasp the idea of pure functions, immutability, and laziness.\nFunctional Paradigms: Monads, functors, and lambdas.\n\n\n\n\n6. Go (Golang)\nSimplicity and efficiency in one package.\n\nGo Essentials: Learn the straightforward syntax, Goroutines, and channels.\nWeb with Go: Build robust web applications using Go’s standard library.\n\n\n\n\n7. C++\nA high-performance language used in system/software development.\n\nC++ Fundamentals: Classes, objects, inheritance, and polymorphism.\nSTL: Dive deep into the Standard Template Library and its offerings.\n\n\n\n\n8. JavaScript\nThe backbone of the modern web.\n\nJavaScript Basics: Understand variables, functions, and DOM manipulation.\nModern JavaScript: Explore ES6 and beyond. Asynchronous operations, Promises, and Fetch API.\nFrameworks: Get to know popular frameworks like React, Vue, and Angular.\n\n\n\n\n9. Software Engineering\nBeyond coding, there’s the art of building software.\n\nVersion Control: Get proficient with Git.\nDesign Patterns: Understand common software design patterns and their applications.\n\n\nSoftware engineering playlist\n\nTesting: Learn the importance of testing and various methodologies, from unit tests to integration tests.\n\n\nClean code playlist\n\n\n\n10. Vim\nThe quintessential text editor.\n\nVim Basics: Understand modes, basic navigation, and editing commands.\n\n\nVim playlist\n\nAdvanced Vim: Buffers, windows, plugins, and custom configurations.\n\n\n\n\n11. Exploring New Programming Languages\nNever stop learning!\n\nResearch: Look for emerging languages and their use cases.\n\n\nCode news playlist\n\nExperiment: Play with sample projects, build simple applications, and test their potential. It doesn’t matter if you’re ignorant to begin with - the most important thing is to discover new things.\n\n\nIgnorant videos playlist\n\n\n\nFinal Words\nRemember, the world of computer science is vast, and this roadmap is just the beginning. Trust in your ability to learn and grow. Every error, every compiled code, and every successfully deployed application is a step forward.\nHappy coding and may your journey through computer science be filled with wonder!"
  },
  {
    "objectID": "blog/Terminal_Basic/index.html",
    "href": "blog/Terminal_Basic/index.html",
    "title": "Learn Terminal Basics",
    "section": "",
    "text": "By Fabrice Hategekimana\nVideo series on terminal automation. We explain why working with the terminal is still a good idea.\nPlaylist here\nNote: video in french, ask in comments for subtitle in your language"
  },
  {
    "objectID": "blog/Terminal_Basic/index.html#first-video-of-the-playlist",
    "href": "blog/Terminal_Basic/index.html#first-video-of-the-playlist",
    "title": "Learn Terminal Basics",
    "section": "First video of the playlist:",
    "text": "First video of the playlist:"
  },
  {
    "objectID": "blog/2023-07-16-you-must-use-rproject/index.html",
    "href": "blog/2023-07-16-you-must-use-rproject/index.html",
    "title": "You must use projects with RStudio!",
    "section": "",
    "text": "Rprojects"
  },
  {
    "objectID": "blog/2023-07-16-you-must-use-rproject/index.html#lets-start-using-projects-right-now",
    "href": "blog/2023-07-16-you-must-use-rproject/index.html#lets-start-using-projects-right-now",
    "title": "You must use projects with RStudio!",
    "section": "Let’s start using projects right now!",
    "text": "Let’s start using projects right now!\nHello budding data scientist! Welcome to another thrilling journey in the enigmatic world of coding. Today, we’re here to chat about a topic so fascinating that it might just outshine your grandmother’s knitting saga. It’s the “Project” feature in RStudio. Yes, you heard it right - it’s time to unravel the secret weapon that will catapult you to new heights in your R journey.\nBefore we embark, let’s clarify something. What’s RStudio, you ask? RStudio is an integrated development environment (IDE) for R, a programming language for statistical computing and graphics. If R is a car, then RStudio is the fancy garage where you improve it up and give it a good polish. But hey, even in the best garages, there are tools that are underused – and ‘projects’ is the underappreciated power drill that’s gathering dust in the corner."
  },
  {
    "objectID": "blog/2023-07-16-you-must-use-rproject/index.html#what-are-projects-in-rstudio",
    "href": "blog/2023-07-16-you-must-use-rproject/index.html#what-are-projects-in-rstudio",
    "title": "You must use projects with RStudio!",
    "section": "What are projects in RStudio?",
    "text": "What are projects in RStudio?\nImagine having a room so messy, you can’t tell the difference between your cat and a fur-lined sweater. Now, think of projects as a giant storage box that allows you to organize your mess (or codes, in our case) and avoid getting your data files, scripts, and outputs mixed up. Remember, no one wants to find a sock in their sandwich!\nSo, in RStudio, a Project is essentially a way to keep all relevant files and specific settings in a single place. Every time you open a Project, RStudio knows exactly where it is, it gets its bearings, and has the sense not to look for your datasets in your downloads or image folder.\nprojects are like trusty sidekicks in the world of programming. They keep our work organized and shielded from the chaos of the digital universe. Just like RStudio’s Rproject, other editors have their own quirky projects. Visual Studio has projects, Sublime Text has projects, and even your grandma’s ancient text editor might have a project feature. projects are like clean bubbles for our code, ensuring it remains unperturbed by the messy dance of files on our computers.\nNote: stop making excuses\nListen up, fellow statisticians! Just because you’re more into numbers than code doesn’t mean you get a free pass on staying desorganized. When you’re knee-deep in dissertations or buried under a pile of homework, you don’t scatter your papers haphazardly around the room, do you? No way! You keep everything in one place, separated from the rest of the chaos. It’s like having a binder for each subject and dividers for each assignment—clear, simple, and organized. projects are no different, my friends! They’re like those fancy binders, but for your digital life. They corral all your code, data, and documentation into one neat little package, shielding it from the tumultuous whirlwind of your computer’s file system."
  },
  {
    "objectID": "blog/2023-07-16-you-must-use-rproject/index.html#why-should-you-use-them",
    "href": "blog/2023-07-16-you-must-use-rproject/index.html#why-should-you-use-them",
    "title": "You must use projects with RStudio!",
    "section": "Why should you use them?",
    "text": "Why should you use them?\n\n1. Stay organized\nImagine having to find the latest script in a haystack of documents or a file saved under a particularly “creative” name like “final_script_v3.2_final_FINAL.R”. A nightmare, isn’t it? With projects, every file you need is in one place - no more, no less. It’s like having a drawer labeled ‘Socks’: you’re not going to find a shirt in there!\n\n\n2. Maintain your working directory\nYour working directory is like your home base in R. With projects, every time you open a particular Project, RStudio sets the working directory to the project’s directory. No more lengthy codes with numerous file paths. It’s like waking up in your own bed every morning, not in the middle of a supermarket.\n\n\n3. Work on multiple projects\nLet’s say you’re juggling different projects, like a statistical analysis of avocado prices and a predictive model for the lottery. You don’t want your avocados messing with your lotto dreams, right? With RStudio projects, you can keep them separate and switch between them seamlessly. It’s like having different rooms for different tasks in your house.\n\n\n4. Share and collaborate\nIf you’ve tried to share your R code with someone who couldn’t run it because they didn’t have their directories set up like yours, then you’ll love projects. They allow other people to run your code without getting tangled up in missing file errors. It’s the equivalent of sending someone a packed lunch with instructions instead of sending them to forage in your fridge.\nIn conclusion, using projects in RStudio is like having a personal assistant who’s a pro at organizing, file managing, and ensuring you don’t get your socks mixed up with your sandwiches. And trust me, in the bustling kitchen of R programming, you’re going to want that!"
  },
  {
    "objectID": "blog/2023-07-16-you-must-use-rproject/index.html#the-cost-of-ignoring-rstudios-projects",
    "href": "blog/2023-07-16-you-must-use-rproject/index.html#the-cost-of-ignoring-rstudios-projects",
    "title": "You must use projects with RStudio!",
    "section": "The cost of ignoring Rstudio’s projects",
    "text": "The cost of ignoring Rstudio’s projects\nWhile coding in R can be a delightful and rewarding experience, there’s a peculiar habit that seems to persist among some developers—a reluctance to embrace the wonders of Rproject. In this blog post, we shall explore the mishaps and frustrations that arise from ignoring Rproject, highlighting the importance of this indispensable tool. So buckle up, and let’s embark on this humorous journey together!\n\n1. Absolute paths and why you should avoid using them\n\nIf you don’t know what absolute paths are\nAh, absolute paths and relative paths, the dynamic duo of file navigation! Think of them as Batman and Robin, but with fewer capes and more directory drama.\nAbsolute paths are like treasure maps that provide the exact coordinates to your desired file or directory. They start from the root of your computer.\nExamples\nWindows:\n\"C:\\Users\\Username\\Documents\\Project\\Folder\\File.txt\"\n\nMac:\n\"/Users/Username/Documents/Project/Folder/File.txt\"\n\nLinux:\n\"/home/Username/Documents/Project/Folder/File.txt\"\nOn the other hand, relative paths are more like giving directions to your friend using landmarks. You guide them based on his current location. It’s like saying, “Go three steps to the right, pass the coffee shop, and you’ll find the mystical file you seek”.\nThe example assume that your current working directory (where your R script or project is located) is at: C:\\Users\\Username\\Documents\\Project\\\nTo navigate to a file named File.txt located in the Folder directory within the current working directory, the relative path would be:\nWindows:\n'Folder\\File.txt'\n\nMac:\n'Folder/File.txt'\n\nLinux:\n'Folder/File.txt'\nIn R, to use relative paths, you simply do the same. If you want more options simply embrace the power of the dots! Use “..” to navigate up one level in the directory hierarchy and “.” to represent the current directory.\nAssume that your current working directory (where your R script or project is located) is at: C:\\Users\\Username\\Documents\\Project\\\nTo navigate to the folder Document which is one level up you would use:\nWindows:\n'..\\'\n\nMac:\n'../'\n\nLinux:\n'../'\nNow, here comes the thrilling part! Inside RStudio, imagine the tab key as a magical wand. When you’re typing a file or directory name within quotation marks, tap that tab key, and behold! A navigation bar appears, offering you a smorgasbord of choices. So, unleash your inner explorer, play with the relative paths, and tap that tab key like a maestro.\n\n\nBack to the dramatic inconvenience of absolute paths\nPicture this: You’ve written a fantastic piece of code in R, filled with wit, charm, and impeccable logic. But there’s a twist—your code contains an absolute file path. Now, when someone else attempts to run your code on their machine, it’s as if you’ve sent them on a mystery tour, wandering through file directories to locate the missing pieces. Be careful! Using the setwd() function (and even worse, the ctrl+shift+H shortcut) doesn’t solve the problem at all. You’re still using absolute paths, which will have to be changed manually by your victim - sorry, I mean colleague! Avoid this problem by utilizing Rproject, which enables you to maintain relative paths and keeps everyone on the same page.\n\n\n\n2. Mixed files\nImagine a symphony performance where each musician plays a different piece entirely. Irritating, right? Well, that’s what happens when you have mixed files scattered across different locations. One file resides in your “Documents” folder, while another hides in the depths of your “Downloads” folder. When collaborating or sharing your code, chaos ensues, leaving others scratching their heads. Rproject corrals all your files into a neat and tidy package, ensuring harmony and preserving the sanity of those who dare to read your code.\n\n\n3. Versioning woes\nAh, the bane of every programmer’s existence—versioning woes. Without Rproject, you enter the realm of time travel bugs, where code from a different era unexpectedly emerges. You find yourself debugging issues that were resolved ages ago, akin to encountering a T-Rex in a modern metropolis. With Rproject, version control systems like Git become your trusty time-traveling companions, allowing you to journey through the annals of code history with ease. Let’s not forget renv, but it will be the topic for another blog post!\n\n\n4. Size doesn’t matter\n“But my project is tiny! I don’t need Rproject,” you protest. Well, dear reader, even the tiniest projects deserve some love and organization. Rproject is not solely for colossal undertakings but a best practice that can save you from future headaches. Plus, it’s a delightful excuse to embark on an adventure, designing your own little universe within RStudio. Alternatively, you can always create a “Test” folder .\nNow, we understand that sometimes you just want to run a quick test, free from the constraints of a formal project. Fear not, intrepid adventurer! You can alter the default path of RStudio and create a whimsical “Test” folder where you can safely experiment and play. It’s a haven for all your quick tests and a sanctuary for spontaneous code escapades (see below in “Test and Project - Yin and Yang”)."
  },
  {
    "objectID": "blog/2023-07-16-you-must-use-rproject/index.html#organize-your-folder-structure",
    "href": "blog/2023-07-16-you-must-use-rproject/index.html#organize-your-folder-structure",
    "title": "You must use projects with RStudio!",
    "section": "Organize your folder structure",
    "text": "Organize your folder structure\nStructure example\n&gt; Code/\n  &gt; C++/\n  &gt; Julia/\n  &gt; Python/\n  &gt; R/\n    &gt; Test/\n    &gt; Project/\n      &gt; Analysis/\n      &gt; Web_scraping/\n      &gt; ...\n\nThe “Code” folder\nStart with a code project to put all your code (regardless of the programming language) together.\n\n\nLanguage-specific folders\nWithin your “Code” folder, it’s time to create language-specific subfolders, each housing a programming language you use. For our R enthusiasts, let’s dive into the “R” folder. Inside, we’ll unveil the secrets of a well-structured R programming project.\n\n\nTest and project - Yin and Yang\nBehold, the dynamic duo of the R folder—the “Test” and “Project” folders. These siblings play distinct roles in your coding journey, ensuring order and clarity.\nThe “Test” folder is your haven for experimentation and ad-hoc code trials. When you’re not working within a formal Rproject, this folder becomes your working directory. It’s here that you can tinker, test, and push the boundaries of your R code without affecting your main projects. To make life easier, let’s make the “Test” folder your permanent default working directory in RStudio.\nIn RStudio, navigate to Tools -&gt; Global Options -&gt; General Look for the Default working directory and select Browse... next to it. Find and select your “Test” folder. Voila! RStudio will now use the “Test” folder as the default working directory whenever you’re not working within an Rproject.\nOn the other hand, the “Project” folder is where the work truly unfolds. When creating an Rproject in RStudio, it will generate a dedicated project folder with all the necessary files and configurations. You have the freedom to structure this folder as you please, organizing your code, data, documentation, and any additional resources. It’s the ideal place to encapsulate the entirety of your project, ensuring that everything stays organized and coherent.\n\n\nCreate RStudio’s Rproject\nNow, you may be wondering, “How do I create an Rproject in RStudio?”\nIn RStudio, navigate to File -&gt; New Project -&gt; New Directory -&gt; Empty Project Choose the location of your Project folder within the “R” folder, provide a name for your project, and click Create Project Voila! RStudio conjures an Rproject, complete with an “.Rproj” file and a wondrous new world awaits.\nIf you already have an existing folder, it is the same way. Click New Project in RStudio’s File menu. Choose Existing Directory click Create Project. The project will take the folder name.\nWithin your newly minted Rproject, you can work correctly while basking in the organized files, version control, and a consistent working directory. RStudio conveniently sets the working directory to the project’s root folder, saving you from horrible absolute paths."
  },
  {
    "objectID": "blog/2023-07-16-you-must-use-rproject/index.html#organize-your-folder-inside-a-rproject",
    "href": "blog/2023-07-16-you-must-use-rproject/index.html#organize-your-folder-inside-a-rproject",
    "title": "You must use projects with RStudio!",
    "section": "Organize your folder inside a Rproject",
    "text": "Organize your folder inside a Rproject\nHow to organize folders inside an Rproject? It’s like cleaning your room, but without the actual physical labor. Keep in mind that there are many way to organize it, here are a few examples:\n# Minimal\n&gt; project_name/\n    &gt; Data/\n    &gt; Script/\n    &gt; Result/\n\n# A more complex example\n&gt; project_name/\n    &gt; Codebook/\n    &gt; Data/\n    &gt; Script/\n        &gt; R/\n        &gt; Rcpp/\n    &gt; Plots/\n\n# R package basic structure\n&gt; package_name/\n    &gt; R/\n    &gt; man/\n    &gt; tests/\n    \n\n# My way\n&gt; project_name\n    &gt; Data_raw\n    &gt; Data_clean\n    &gt; Script\n        &gt; Fun\n    &gt; Report\n    &gt; Results\n        &gt; Figures\n        &gt; Tables"
  },
  {
    "objectID": "blog/2023-07-16-you-must-use-rproject/index.html#conclusion",
    "href": "blog/2023-07-16-you-must-use-rproject/index.html#conclusion",
    "title": "You must use projects with RStudio!",
    "section": "Conclusion",
    "text": "Conclusion\nAs R programming enthusiasts, we’ve ventured through its intricacies with Rproject, an essential tool that safeguards against absolute paths, file mix-ups, and versioning issues. All projects, large or small, merit organization and care. We’ve explored the allure of folder structures and Rproject creation, finding peace in structured coding by managing specific subfolders and enhancing the relationship between “Test” and “Project” folders. RStudio assists us in this journey, simplifying the creation of Rprojects, thus promoting productivity and precision. Fellow coders, let’s advance with our folder structures as elegant as our code!"
  },
  {
    "objectID": "blog/Python_Fundamentals/index.html",
    "href": "blog/Python_Fundamentals/index.html",
    "title": "Python Basics",
    "section": "",
    "text": "By Fabrice Hategekimana\nGoal: To initiate people who would like to start with Python by starting with the basics or the basics or people who want to reinforce their knowledge by going through important concepts in programming.\nPlaylist here\nNote: video in french, ask in comments for subtitle in your language"
  },
  {
    "objectID": "blog/Python_Fundamentals/index.html#first-video-of-the-playlist",
    "href": "blog/Python_Fundamentals/index.html#first-video-of-the-playlist",
    "title": "Python Basics",
    "section": "First video of the playlist:",
    "text": "First video of the playlist:"
  },
  {
    "objectID": "blog/My-problem-with-python/index.html",
    "href": "blog/My-problem-with-python/index.html",
    "title": "My problem with Python",
    "section": "",
    "text": "Today, we’re diving into a rather controversial topic: ‘My problem with Python.’ But before you raise your eyebrows, let me clarify: It’s not quite what you think. So, let’s unravel this Python conundrum.\n\n\n\nLet’s start by setting the record straight. Python, the language, is pretty fantastic. It’s versatile, boasts a treasure trove of data science libraries, and guess what? It was my first programming language. That’s right, Python and I go way back to my days as a stats newbie, looking for something more robust than STATA.\nBack then, it was a showdown between R and Python for me. The online world seemed to be in a Python frenzy. Everywhere I looked, articles, tutorials, you name it – they all seemed to be chanting ‘Python, Python, Python!’ So, naturally, I jumped on the bandwagon. Python became my go-to for data science, my knight in shining armor, if you will.\n\n\n\nBut then, plot twist! During my master’s, I was forced to learn R. And guess what? I was blown away. R was like this hidden gem, perfectly cut for data analysis. Slowly but surely, R started replacing Python in my data science toolkit. That got me thinking: Were those Python-preaching articles a bit too… biased?\nAs I dug deeper, it became clear. Many of those Python articles had misunderstood R, or worse, underestimated it. The craziest thing about all this is that many of these articles have literally pointed out things that aren’t true, such as the fact that R doesn’t have any good machine learning or visualization libraries. Also, some were comparing plain R with Python + Pandas + Numpy + ScikitLearn + TensorFlow. This realization hit me hard.\nThese articles and videos were much more subjective than they seemed in their choice of examples (sometimes these examples were already out of date) and concretely little effort was made in searching in R.\nLatter I ventured further, exploring languages like Julia and Nim. I needed performance, I craved compilation, and I was hunting for application. These new languages opened my eyes to a world beyond Python. Yet, Python remained in my arsenal, like an old friend you can’t let go of.\n\n\n\nLike all programming languages, Python has its shortcomings, which I can easily mention. These flaws become obvious when I compare it to other programming languages I use: - It’s slow (not compiled) - Has a huge dependency on C for heavy tasks - Possible to create desktop applications, but inappropriate - Has no built-in linear algebra functionality - No integrated dataframe management functionality - No integrated statistics or machine learning functionality - Object-oriented doesn’t integrate well with any of the above functionalities (Numpy and Pandas for instance feel weird to use personally)\nAnd the list goes on. But these aren’t my problem: no programming language is perfect.\nSo, here’s my ‘problem’ with Python: It’s not the language itself, but the popularity bias surrounding it. This bias blinds us, leading many to overlook alternatives that might be better suited for certain tasks. It creates a bubble, where Python is the be-all and end-all, and other languages are mere afterthoughts. This bubble isn’t just limiting; it’s dangerous for beginners.\nIn concrete terms, thanks to its versatility, Python can become a comfort zone from which it’s hard to escape. But focusing on just one language and ignoring the alternatives can lead to a number of problems: - Performance: In the case of languages like Python, performance is limited and you have to rely on libraries that depend on other, more powerful languages for specific uses (C/C++). What’s more, when it comes to writing high-performance programs in their own right, it’s a bad idea to write them in Python or other supposedly faster alternatives (Pypy or Cython) when you could be using other more powerful and almost equally easy-to-use languages like Julia, Mojo or Nim. This solve the two language problem in a better way. - Ecosystem: Even if Python could virtually be used in many circumstances, there are languages that have ecosystems better suited to certain tasks, such as JavaScript for Web development, R for statistics, Kotlin for application development or Rust for system development. - Career: It’s possible to make a career out of a single programming language. The chances of working with just one language seem high in a rare programming language. In the case of a language like Python (which everyone uses), it’s more appropriate to have another language to back up your skills. Even in a field like data science, it’s very often required to know how to use SQL or R in addition to Python. What’s more, it’s not only machine learning skills that are required, but also knowledge of statistics, architecture (AWS, Azure, etc.) and other skills. What’s more, having more than one language means more career opportunities.\nDon’t get me wrong, Python is great. I still use it for many projects (nice for scripting). But the key takeaway? Don’t get cloistered in a single language. Explore, experiment, and most importantly, keep an open mind. Each language has its strengths, its charm, and yes, its quirks.\nSo, whether you’re a Python pro, an R enthusiast, or a Julia fan, remember: the world of programming is vast and varied. Embrace it. And hey, who knows? You might just find your new programming soulmate.\n\n\n\nSo, we’ve talked about the language, its strengths, and the bias around it. Now, let’s explore the darker side of popularity bias and the hit-parade effect in programming.\nYou see, popularity bias isn’t unique to Python or even programming. It’s a widespread phenomenon, where popularity feeds more popularity, often overshadowing other equally or more deserving options. In music, it’s the hit-parade effect – the more a song is played, the more popular it becomes, regardless of its quality.\nThis hit-parade effect seeps into the world of programming too. Python, no doubt a great language, has benefitted from this. Its popularity has skyrocketed, not solely because of its merits but partly due to a self-perpetuating cycle of popularity. It’s like a snowball rolling downhill, getting bigger and bigger.\nBut why is this dangerous? Well, think about it. When one language dominates the spotlight, others are left in the shadows, no matter how good they might be. This creates a monoculture in programming, where diversity of thought, approach, and solution is stifled. We start seeing every problem as a nail, and Python as the only hammer we have.\nThis is less a problem for large companies using several programming languages, but rather for programmers engaged in a war to find the best programming language, or simply looking for the easy way out.\nBut let’s not get it twisted. Python is a fantastic language and a safe choice. Its simplicity, vast libraries, and community support are unparalleled. It’s like the friendly neighborhood of programming languages – welcoming and accessible (take notes Nim). But, part of its overwhelming popularity is definitely fueled by this hit-parade effect: It’s taught everywhere (even compulsary), and it’s virtually impossible not to have heard of it.\nPython is pushed by companies, articles, tutorials, over-enthusiastic Pythonistas, hype for Artificial Intelligence and yes, even content like this one. The danger lies in becoming so enamored with Python that we become blind to other languages’ potential. Remember, every language was designed with a purpose in mind, and sometimes, that purpose aligns perfectly with your specific needs.\nLike JavaScript, Python represents the piano in music; everyone knows how to play it in addition to their instrument of choice. But in the end, an orchestra is made up of a diversity of instruments, and without this diversity, there could be no beautiful symphony.\nSo, here’s my call to action: Explore. Be curious. Don’t let popularity bias dictate your choice of programming language. Dive into R for statistical analysis, Julia for high-performance computing, or even Rust for systems programming. The landscape is rich and diverse – a treasure trove waiting to be explored.\nIn the end, it’s about using the right tool for the job, not the most popular one. It’s about innovation, exploration, and sometimes, about going against the grain. So, whether you’re a seasoned coder or a newbie, remember: there’s a whole world beyond Python, and it’s just waiting for you to discover it.\n\n\n\nNow, let’s tackle the age-old debate: R versus Python. It’s a question as old as time in the data science world. Which one should you choose?\nWhen someone asks me, “Should I go for R or Python?” I don’t just throw an answer at them. Instead, I turn into a bit of a detective. I ask questions about their needs, motivations, and the environment they’re working in. Who around them is using these languages? What kind of analysis/code are they dealing with? What’s their end goal?\nYou see, it’s not about which language is “the best”. It’s about which language is better for you, for your specific situation. Python, with its simplicity and vast libraries, might be the go-to for general-purpose programming and machine learning. R, on the other hand, shines in statistical analysis and has fantastic packages for data visualization.\n\n\nBut here’s the kicker: I always suggest learning both (not at the same time of course). That’s right. Why limit yourself to one when you can have the best of both worlds? That’s what I wish someone had told me back in the day. Ask “Which one should I learn first?” not “Which one is the best?” And base your decision on your needs, not on a false competition filled with bias and misinformation. It’s easier to choose when you don’t feel that the choice you’ve made will impact your whole life. Knowing that you can learn the one you want next based on your needs is liberating.\nBeing a programming polyglot opens up a world of possibilities. It’s like being a chef with access to a global pantry of ingredients. You can pick and choose the best tool for each recipe, for each task at hand. And when you’re ready, why stop at R and Python? Dive into Julia for high-performance tasks, or explore SQL for database management. The more languages you know, the more versatile and capable you become.\nSo, dear listeners, embrace the journey of learning. Be open to expanding your skillset. Remember, in the rapidly evolving world of programming, being adaptable and versatile is key. And most importantly, never stop being curious. Who knows what incredible solutions you’ll discover when you step out of your comfort zone?\n\n\n\nI would like to offer some insights for Python enthusiasts who might not fully appreciate the capabilities of R. My observations suggest that there’s a prevalent misconception among many Python users about R’s utility and scope. This misconception seems to originate from two main factors. Firstly, in academic settings, R is typically introduced as a statistical tool, primarily taught by experts in mathematics or statistics. This approach often limits the exposure to R’s potential in data science, contrasting with Python, which is predominantly showcased in this context. Such a skewed presentation could contribute to the underestimation of R. Secondly, Python’s overwhelming presence on the internet, particularly in data science forums, reinforces its popularity. This larger user base naturally leads to a dominance in online discussions, overshadowing R’s presence. It’s also important to address a common oversight regarding the application of programming languages in industry settings.\nWhile the usage of Python in major companies like Google and Amazon is well-publicized, it’s a misconception to assume it’s their exclusive choice for data science tasks. In reality, many large corporations also incorporate R in their data science operations. Such as Airbnb, Amazon Web Service Ebay, Facebook, Google, Microsoft, Mozilla, Netflix, Twitter or Uber to name a few (source). Even companies made the choice to work mainly with R like Thinkr and Appsilon.\nIn fact R has solid libraries and tools for data science task: Quarto or Rmarkdown notebook for versatile publication, the Tidyverse ecosystem for data wrangling, the Tidymodels ecosystem or Caret or Mlr3 for machine learning, the Torch ecosystem for deep learning or Shiny for web application to name some famous ones.\nThe key takeaway is that R is far from being a dated, esoteric, and impenetrable statistical language confined to academic circles and favored only by nearing-retirement professors. Rather, it is a vibrant and dynamic language supported by a robust community deeply invested in data science. R is already a staple in production environments, actively used for a variety of real-world applications. The intent here is not to argue that R matches Python in terms of widespread usage – they are distinct languages with different strengths – but rather to challenge and correct the misconceptions some Python users might hold about R. This perspective aims to broaden the understanding and appreciation of what R brings to the table in the realm of data science."
  },
  {
    "objectID": "blog/My-problem-with-python/index.html#introduction",
    "href": "blog/My-problem-with-python/index.html#introduction",
    "title": "My problem with Python",
    "section": "",
    "text": "Today, we’re diving into a rather controversial topic: ‘My problem with Python.’ But before you raise your eyebrows, let me clarify: It’s not quite what you think. So, let’s unravel this Python conundrum."
  },
  {
    "objectID": "blog/My-problem-with-python/index.html#why-learn-python",
    "href": "blog/My-problem-with-python/index.html#why-learn-python",
    "title": "My problem with Python",
    "section": "",
    "text": "Let’s start by setting the record straight. Python, the language, is pretty fantastic. It’s versatile, boasts a treasure trove of data science libraries, and guess what? It was my first programming language. That’s right, Python and I go way back to my days as a stats newbie, looking for something more robust than STATA.\nBack then, it was a showdown between R and Python for me. The online world seemed to be in a Python frenzy. Everywhere I looked, articles, tutorials, you name it – they all seemed to be chanting ‘Python, Python, Python!’ So, naturally, I jumped on the bandwagon. Python became my go-to for data science, my knight in shining armor, if you will."
  },
  {
    "objectID": "blog/My-problem-with-python/index.html#but",
    "href": "blog/My-problem-with-python/index.html#but",
    "title": "My problem with Python",
    "section": "",
    "text": "But then, plot twist! During my master’s, I was forced to learn R. And guess what? I was blown away. R was like this hidden gem, perfectly cut for data analysis. Slowly but surely, R started replacing Python in my data science toolkit. That got me thinking: Were those Python-preaching articles a bit too… biased?\nAs I dug deeper, it became clear. Many of those Python articles had misunderstood R, or worse, underestimated it. The craziest thing about all this is that many of these articles have literally pointed out things that aren’t true, such as the fact that R doesn’t have any good machine learning or visualization libraries. Also, some were comparing plain R with Python + Pandas + Numpy + ScikitLearn + TensorFlow. This realization hit me hard.\nThese articles and videos were much more subjective than they seemed in their choice of examples (sometimes these examples were already out of date) and concretely little effort was made in searching in R.\nLatter I ventured further, exploring languages like Julia and Nim. I needed performance, I craved compilation, and I was hunting for application. These new languages opened my eyes to a world beyond Python. Yet, Python remained in my arsenal, like an old friend you can’t let go of."
  },
  {
    "objectID": "blog/My-problem-with-python/index.html#my-problem-with-python-1",
    "href": "blog/My-problem-with-python/index.html#my-problem-with-python-1",
    "title": "My problem with Python",
    "section": "",
    "text": "Like all programming languages, Python has its shortcomings, which I can easily mention. These flaws become obvious when I compare it to other programming languages I use: - It’s slow (not compiled) - Has a huge dependency on C for heavy tasks - Possible to create desktop applications, but inappropriate - Has no built-in linear algebra functionality - No integrated dataframe management functionality - No integrated statistics or machine learning functionality - Object-oriented doesn’t integrate well with any of the above functionalities (Numpy and Pandas for instance feel weird to use personally)\nAnd the list goes on. But these aren’t my problem: no programming language is perfect.\nSo, here’s my ‘problem’ with Python: It’s not the language itself, but the popularity bias surrounding it. This bias blinds us, leading many to overlook alternatives that might be better suited for certain tasks. It creates a bubble, where Python is the be-all and end-all, and other languages are mere afterthoughts. This bubble isn’t just limiting; it’s dangerous for beginners.\nIn concrete terms, thanks to its versatility, Python can become a comfort zone from which it’s hard to escape. But focusing on just one language and ignoring the alternatives can lead to a number of problems: - Performance: In the case of languages like Python, performance is limited and you have to rely on libraries that depend on other, more powerful languages for specific uses (C/C++). What’s more, when it comes to writing high-performance programs in their own right, it’s a bad idea to write them in Python or other supposedly faster alternatives (Pypy or Cython) when you could be using other more powerful and almost equally easy-to-use languages like Julia, Mojo or Nim. This solve the two language problem in a better way. - Ecosystem: Even if Python could virtually be used in many circumstances, there are languages that have ecosystems better suited to certain tasks, such as JavaScript for Web development, R for statistics, Kotlin for application development or Rust for system development. - Career: It’s possible to make a career out of a single programming language. The chances of working with just one language seem high in a rare programming language. In the case of a language like Python (which everyone uses), it’s more appropriate to have another language to back up your skills. Even in a field like data science, it’s very often required to know how to use SQL or R in addition to Python. What’s more, it’s not only machine learning skills that are required, but also knowledge of statistics, architecture (AWS, Azure, etc.) and other skills. What’s more, having more than one language means more career opportunities.\nDon’t get me wrong, Python is great. I still use it for many projects (nice for scripting). But the key takeaway? Don’t get cloistered in a single language. Explore, experiment, and most importantly, keep an open mind. Each language has its strengths, its charm, and yes, its quirks.\nSo, whether you’re a Python pro, an R enthusiast, or a Julia fan, remember: the world of programming is vast and varied. Embrace it. And hey, who knows? You might just find your new programming soulmate."
  },
  {
    "objectID": "blog/My-problem-with-python/index.html#popularity-bias",
    "href": "blog/My-problem-with-python/index.html#popularity-bias",
    "title": "My problem with Python",
    "section": "",
    "text": "So, we’ve talked about the language, its strengths, and the bias around it. Now, let’s explore the darker side of popularity bias and the hit-parade effect in programming.\nYou see, popularity bias isn’t unique to Python or even programming. It’s a widespread phenomenon, where popularity feeds more popularity, often overshadowing other equally or more deserving options. In music, it’s the hit-parade effect – the more a song is played, the more popular it becomes, regardless of its quality.\nThis hit-parade effect seeps into the world of programming too. Python, no doubt a great language, has benefitted from this. Its popularity has skyrocketed, not solely because of its merits but partly due to a self-perpetuating cycle of popularity. It’s like a snowball rolling downhill, getting bigger and bigger.\nBut why is this dangerous? Well, think about it. When one language dominates the spotlight, others are left in the shadows, no matter how good they might be. This creates a monoculture in programming, where diversity of thought, approach, and solution is stifled. We start seeing every problem as a nail, and Python as the only hammer we have.\nThis is less a problem for large companies using several programming languages, but rather for programmers engaged in a war to find the best programming language, or simply looking for the easy way out.\nBut let’s not get it twisted. Python is a fantastic language and a safe choice. Its simplicity, vast libraries, and community support are unparalleled. It’s like the friendly neighborhood of programming languages – welcoming and accessible (take notes Nim). But, part of its overwhelming popularity is definitely fueled by this hit-parade effect: It’s taught everywhere (even compulsary), and it’s virtually impossible not to have heard of it.\nPython is pushed by companies, articles, tutorials, over-enthusiastic Pythonistas, hype for Artificial Intelligence and yes, even content like this one. The danger lies in becoming so enamored with Python that we become blind to other languages’ potential. Remember, every language was designed with a purpose in mind, and sometimes, that purpose aligns perfectly with your specific needs.\nLike JavaScript, Python represents the piano in music; everyone knows how to play it in addition to their instrument of choice. But in the end, an orchestra is made up of a diversity of instruments, and without this diversity, there could be no beautiful symphony.\nSo, here’s my call to action: Explore. Be curious. Don’t let popularity bias dictate your choice of programming language. Dive into R for statistical analysis, Julia for high-performance computing, or even Rust for systems programming. The landscape is rich and diverse – a treasure trove waiting to be explored.\nIn the end, it’s about using the right tool for the job, not the most popular one. It’s about innovation, exploration, and sometimes, about going against the grain. So, whether you’re a seasoned coder or a newbie, remember: there’s a whole world beyond Python, and it’s just waiting for you to discover it."
  },
  {
    "objectID": "blog/My-problem-with-python/index.html#conclusion-python-vs-r",
    "href": "blog/My-problem-with-python/index.html#conclusion-python-vs-r",
    "title": "My problem with Python",
    "section": "",
    "text": "Now, let’s tackle the age-old debate: R versus Python. It’s a question as old as time in the data science world. Which one should you choose?\nWhen someone asks me, “Should I go for R or Python?” I don’t just throw an answer at them. Instead, I turn into a bit of a detective. I ask questions about their needs, motivations, and the environment they’re working in. Who around them is using these languages? What kind of analysis/code are they dealing with? What’s their end goal?\nYou see, it’s not about which language is “the best”. It’s about which language is better for you, for your specific situation. Python, with its simplicity and vast libraries, might be the go-to for general-purpose programming and machine learning. R, on the other hand, shines in statistical analysis and has fantastic packages for data visualization.\n\n\nBut here’s the kicker: I always suggest learning both (not at the same time of course). That’s right. Why limit yourself to one when you can have the best of both worlds? That’s what I wish someone had told me back in the day. Ask “Which one should I learn first?” not “Which one is the best?” And base your decision on your needs, not on a false competition filled with bias and misinformation. It’s easier to choose when you don’t feel that the choice you’ve made will impact your whole life. Knowing that you can learn the one you want next based on your needs is liberating.\nBeing a programming polyglot opens up a world of possibilities. It’s like being a chef with access to a global pantry of ingredients. You can pick and choose the best tool for each recipe, for each task at hand. And when you’re ready, why stop at R and Python? Dive into Julia for high-performance tasks, or explore SQL for database management. The more languages you know, the more versatile and capable you become.\nSo, dear listeners, embrace the journey of learning. Be open to expanding your skillset. Remember, in the rapidly evolving world of programming, being adaptable and versatile is key. And most importantly, never stop being curious. Who knows what incredible solutions you’ll discover when you step out of your comfort zone?\n\n\n\nI would like to offer some insights for Python enthusiasts who might not fully appreciate the capabilities of R. My observations suggest that there’s a prevalent misconception among many Python users about R’s utility and scope. This misconception seems to originate from two main factors. Firstly, in academic settings, R is typically introduced as a statistical tool, primarily taught by experts in mathematics or statistics. This approach often limits the exposure to R’s potential in data science, contrasting with Python, which is predominantly showcased in this context. Such a skewed presentation could contribute to the underestimation of R. Secondly, Python’s overwhelming presence on the internet, particularly in data science forums, reinforces its popularity. This larger user base naturally leads to a dominance in online discussions, overshadowing R’s presence. It’s also important to address a common oversight regarding the application of programming languages in industry settings.\nWhile the usage of Python in major companies like Google and Amazon is well-publicized, it’s a misconception to assume it’s their exclusive choice for data science tasks. In reality, many large corporations also incorporate R in their data science operations. Such as Airbnb, Amazon Web Service Ebay, Facebook, Google, Microsoft, Mozilla, Netflix, Twitter or Uber to name a few (source). Even companies made the choice to work mainly with R like Thinkr and Appsilon.\nIn fact R has solid libraries and tools for data science task: Quarto or Rmarkdown notebook for versatile publication, the Tidyverse ecosystem for data wrangling, the Tidymodels ecosystem or Caret or Mlr3 for machine learning, the Torch ecosystem for deep learning or Shiny for web application to name some famous ones.\nThe key takeaway is that R is far from being a dated, esoteric, and impenetrable statistical language confined to academic circles and favored only by nearing-retirement professors. Rather, it is a vibrant and dynamic language supported by a robust community deeply invested in data science. R is already a staple in production environments, actively used for a variety of real-world applications. The intent here is not to argue that R matches Python in terms of widespread usage – they are distinct languages with different strengths – but rather to challenge and correct the misconceptions some Python users might hold about R. This perspective aims to broaden the understanding and appreciation of what R brings to the table in the realm of data science."
  },
  {
    "objectID": "blog/R_road_map/Index.html",
    "href": "blog/R_road_map/Index.html",
    "title": "R Road Map",
    "section": "",
    "text": "R Learning Roadmap for Budding Data Scientists and Statisticians\nHey there, future statistician and data scientist! Welcome to the wonderful world of R, a programming language specifically designed for data analysis, statistics, and graphical representation. Whether you’re a statistician looking to expand your toolkit or an aspiring data scientist eager to dive into data manipulation and visualization, this roadmap is your guiding star.\nNote: This blog will probably evolve with the new content we are providing on YouTube.\n\n\nWhy R?\nR is a flexible and powerful tool. Developed by statisticians, for statisticians, it boasts an incredible ecosystem of packages, making it one of the most popular choices for data-driven tasks. Plus, it’s free and open source!\n\n\n\nR Learning Roadmap\nThis roadmap is divided into four main categories:\n\nFoundation\nIntermediate Techniques\nAdvanced Analysis\nSpecialized Areas\n\nLet’s break these down!\n\n\n1. Foundation\nBefore you can master the intricate techniques of data science and statistics in R, you need to get the basics right.\nBase: Learn how to install R and Rstudio, the basics of synthaxis, the first types and how to create functions.\n\nBase R playlist\nManagment: Learn how to use the language and its dedicated editor to their full potential, with tips and basics.\n\nStatistic with R playlist\nStatistic: Learn the basics of statistical analysis and conduct your first study from A to Z using all the tools available in R.\n\n\n\n\n2. Intermediate Techniques\nWith the basics in hand, let’s delve deeper!\nData workflow: Learn the basics of the Tidyverse and master data manipulation. You can learn it at any level.\n\nTidyverse playlist\n\nData Manipulation: Master the dplyr package for tasks like filtering, arranging, and summarizing data. You’ll love the pipe (%&gt;%) operator!\nAdvanced Visualization: Get to know ggplot2, the most popular visualization package in R. The Grammar of Graphics will revolutionize how you think about plotting.\n\n\nggplot2 playlist\n\nData Cleaning: tidyr is your friend here. Learn techniques like pivot, separate, and unite.\n\n\n\n\n3. Advanced Analysis\nNow, let’s dive deep!\n\nAdvanced Statistical Modeling: Explore more advanced techniques like multiple regression, logistic regression, and ANOVA.\nMachine Learning: With packages from the Tidymodel, dive into classification, clustering, and regression models.\nTime Series Analysis: Use packages like forecast for time series decomposition and forecasting.\nReporting: Learn how to knit your R Markdown documents into interactive HTML, PDFs, and slideshows to share your findings.\n\n\n\n\n4. Specialized Areas\nDepending on your interest, there’s always more to explore:\n\nText Mining: With quanteda and tidytext, dive into the world of NLP.\nGeospatial Analysis: sf and leaflet will help you work with spatial data.\nBioinformatics: If you’re into biology, Bioconductor provides tools for bioinformatics.\nShiny Apps: Turn your analyses into interactive web applications with shiny.\n\n\n\n\n\nFinal Words\nRemember, the journey of learning R, like all things, is best taken one step at a time. You might feel overwhelmed initially, but trust the process. With each line of code, each plot, and each model, you’re getting better.\nHappy coding, and here’s to your data-driven adventures with R!"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Contact\n\n\nContact us at info@wedata.ch."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "tl;dr\n\nWe are an association of the University of Geneva that aims to share its passion for data science and computer science. We have a strong interest in statistics and computational methods. Initially, the association’s target audience was people in the social sciences but we quickly expanded into other fields of research.  More concretely, the association aims to achieve its objectives by creating freely-accessible educational content on its platforms in a variety of forms: YouTube videos, blog posts, exercises, etc. The association co-organizes the R-Lunches, a series of events related to R in which various speakers present topics related to the R language. The association tries to keep abreast of and participate as often as possible in digital initiatives at the University of Geneva."
  },
  {
    "objectID": "blog/2024-02-11_R_and_Python/index.html",
    "href": "blog/2024-02-11_R_and_Python/index.html",
    "title": "R and Python side-by-side for data wrangling",
    "section": "",
    "text": "After seeing many language wars style posts about  vs  and the sort of comparisons being made, I realized that there aren’t many helpful side-by-sides that show you how to do x in y language (and vice versa).\n\nI decided to try and see if I could contribute something to the discourse. I’m not really trying to reinvent an analysis wheel and just want to focus on the how something is accomplished from one language to the other so I’m pulling from a few sources to just have some code to translate using the same data for both languages.\nSince polars is new to me and I like learning new things, I’m using it for the examples"
  },
  {
    "objectID": "blog/2024-02-11_R_and_Python/index.html#data",
    "href": "blog/2024-02-11_R_and_Python/index.html#data",
    "title": "R and Python side-by-side for data wrangling",
    "section": "Data",
    "text": "Data\n\nData was obtained from the gapminder R package and written to parquet via R’s arrow::write_parquet for better interoporability between R and Python. Additionally, the size is low enough to pull the data as parquet from my GitHub repo."
  },
  {
    "objectID": "blog/2024-02-11_R_and_Python/index.html#packages",
    "href": "blog/2024-02-11_R_and_Python/index.html#packages",
    "title": "R and Python side-by-side for data wrangling",
    "section": " packages",
    "text": "packages\n\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(arrow, include.only = \"read_parquet\")\nlibrary(magrittr, include.only = \"%&lt;&gt;%\")\n\ngapminder &lt;- read_parquet(\"gapminder.parquet\")"
  },
  {
    "objectID": "blog/2024-02-11_R_and_Python/index.html#libraries",
    "href": "blog/2024-02-11_R_and_Python/index.html#libraries",
    "title": "R and Python side-by-side for data wrangling",
    "section": " libraries",
    "text": "libraries\n\n\nimport polars as pl\nimport plotly.express as px\n\ngapminder = pl.read_parquet(\"gapminder.parquet\")\n\ngapminder = (gapminder\n  .with_columns([\n    pl.col(\"country\").cast(pl.Utf8),\n    pl.col(\"continent\").cast(pl.Utf8),\n    pl.col(\"region\").cast(pl.Utf8)  \n  ])\n)\n\n return top 10 rows in R\n\ngapminder |&gt; head(10)\n\n\n  \n\n\n\n get quick info on the data with dplyr::glimpse()\n\ngapminder |&gt; glimpse()\n\nRows: 10,545\nColumns: 9\n$ country          &lt;fct&gt; \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             &lt;int&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,…\n$ infant_mortality &lt;dbl&gt; 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.…\n$ life_expectancy  &lt;dbl&gt; 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8…\n$ fertility        &lt;dbl&gt; 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,…\n$ population       &lt;dbl&gt; 1636054, 11124892, 5270844, 54681, 20619075, 1867396,…\n$ gdp              &lt;dbl&gt; NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778…\n$ continent        &lt;fct&gt; Europe, Africa, Africa, Americas, Americas, Asia, Ame…\n$ region           &lt;fct&gt; Southern Europe, Northern Africa, Middle Africa, Cari…\n\n\n return top 10 rows in Python\n\ngapminder.head(10)\n\n\n\n\n  \n\n\n\n get quick info on the data with pandas’s info DataFrame method\n\ngapminder.to_pandas().info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10545 entries, 0 to 10544\nData columns (total 9 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   country           10545 non-null  object \n 1   year              10545 non-null  int32  \n 2   infant_mortality  9092 non-null   float64\n 3   life_expectancy   10545 non-null  float64\n 4   fertility         10358 non-null  float64\n 5   population        10360 non-null  float64\n 6   gdp               7573 non-null   float64\n 7   continent         10545 non-null  object \n 8   region            10545 non-null  object \ndtypes: float64(5), int32(1), object(3)\nmemory usage: 700.4+ KB\n\n\nThis will come back later, but it’s very easy to move your polars data into a pandas DataFrame.\n\n\nSri Lanka VS Turkey\n\n simple dplyr::filter and dplyr::select\n\ngapminder |&gt;\n  filter(year == \"2015\", country %in% c(\"Sri Lanka\", \"Turkey\")) |&gt;\n  select(country, infant_mortality)\n\n\n  \n\n\n\n simple filter and select method chain\n\n(gapminder\n  .filter(\n    (pl.col(\"year\") == 2015) & \n    (pl.col(\"country\").is_in([\"Sri Lanka\", \"Turkey\"]))) \n  .select([\"country\", \"infant_mortality\"])\n) \n\n\n\n\n  \n\n\n\nThis is where you can start to see how powerful polars can be in terms of the way it handles lazy evaluation. One of the reasons dplyr is so expressive and intuitive (at least in my view) is due in large part to the way it handles lazy evaluation. For people that are tired of constantly needing to refer to the data and column in pandas will likely rejoice at polars.col!\n\n\n\nLet’s just compare them all at once\n\n same strategy; more countries\n\ngapminder |&gt;\n  filter(\n    year == \"2015\", \n    country %in% c(\n      \"Sri Lanka\", \"Turkey\", \"Poland\", \"South Korea\",\n      \"Malaysia\", \"Russia\", \"Pakistan\", \"Vietnam\",\n      \"Thailand\", \"South Africa\")) |&gt;\n  select(country, infant_mortality) |&gt;\n  arrange(desc(infant_mortality))\n\n\n  \n\n\n\n same as above\n\n(gapminder\n  .filter(\n    (pl.col(\"year\") == 2015) & \n    (pl.col(\"country\").is_in([\n      \"Sri Lanka\", \"Turkey\", \"Poland\", \"South Korea\", \n      \"Poland\", \"South Korea\",\"Malaysia\", \"Russia\", \n      \"Pakistan\", \"Vietnam\", \"Thailand\", \"South Africa\"]))) \n  .select([\"country\", \"infant_mortality\"])\n  .sort(\"infant_mortality\", descending = True)\n)"
  },
  {
    "objectID": "blog/2024-02-11_R_and_Python/index.html#hans-roslings-quiz",
    "href": "blog/2024-02-11_R_and_Python/index.html#hans-roslings-quiz",
    "title": "AR and Python side-by-side for data wrangling",
    "section": "Hans Rosling’s quiz",
    "text": "Hans Rosling’s quiz\n\nFollowing along Hans Rosling’s New Insights on Poverty video, we’re going to answer the questions he poses in connection to child mortality rates in 2015. He asks, which pairs do you think are most similar?\n\nSri Lanka or Turkey\nPoland or South Korea\nMalaysia or Russia\nPakistan or Vietnam\nThailand or South Africa\n\n\n\nSri Lanka or Turkey\n\n simple dplyr::filter and dplyr::select\n\ngapminder |&gt;\n  filter(year == \"2015\", country %in% c(\"Sri Lanka\", \"Turkey\")) |&gt;\n  select(country, infant_mortality)\n\n\n  \n\n\n\n simple filter and select method chain\n\n(gapminder\n  .filter(\n    (pl.col(\"year\") == 2015) & \n    (pl.col(\"country\").is_in([\"Sri Lanka\", \"Turkey\"]))) \n  .select([\"country\", \"infant_mortality\"])\n) \n\n\n\n\n  \n\n\n\nThis is where you can start to see how powerful polars can be in terms of the way it handles lazy evaluation. One of the reasons dplyr is so expressive and intuitive (at least in my view) is due in large part to the way it handles lazy evaluation. For people that are tired of constantly needing to refer to the data and column in pandas will likely rejoice at polars.col!\n\n\n\nLet’s just compare them all at once\n\n same strategy; more countries\n\ngapminder |&gt;\n  filter(\n    year == \"2015\", \n    country %in% c(\n      \"Sri Lanka\", \"Turkey\", \"Poland\", \"South Korea\",\n      \"Malaysia\", \"Russia\", \"Pakistan\", \"Vietnam\",\n      \"Thailand\", \"South Africa\")) |&gt;\n  select(country, infant_mortality) |&gt;\n  arrange(desc(infant_mortality))\n\n\n  \n\n\n\n same as above\n\n(gapminder\n  .filter(\n    (pl.col(\"year\") == 2015) & \n    (pl.col(\"country\").is_in([\n      \"Sri Lanka\", \"Turkey\", \"Poland\", \"South Korea\", \n      \"Poland\", \"South Korea\",\"Malaysia\", \"Russia\", \n      \"Pakistan\", \"Vietnam\", \"Thailand\", \"South Africa\"]))) \n  .select([\"country\", \"infant_mortality\"])\n  .sort(\"infant_mortality\", descending = True)\n)"
  },
  {
    "objectID": "blog/2024-02-11_R_and_Python/index.html#aggregates",
    "href": "blog/2024-02-11_R_and_Python/index.html#aggregates",
    "title": "R and Python side-by-side for data wrangling",
    "section": "Aggregates",
    "text": "Aggregates\n\n grouping and taking an average\n\ngapminder |&gt;\n  group_by(continent) |&gt;\n  summarise(mean_life_expectancy = mean(life_expectancy) |&gt;\n              round(2), .groups = \"keep\")\n\n\n  \n\n\n\n now with polars\n\n(gapminder\n  .group_by(\"continent\")\n  .agg([\n    (pl.col(\"life_expectancy\")\n        .mean().\n        round(2).\n        alias(\"mean_life_expectancy\"))\n    ])\n  .sort(\"continent\")\n) \n\n\n\n\n  \n\n\n\n\nI think this is probably a good enough intro to how you’d generally do things. Filtering, and aggregatingare probably the most foundational and this could already get you started in another language without as much headache   :::"
  },
  {
    "objectID": "blog/2024-02-11_R_and_Python/index.html#with-conditionals",
    "href": "blog/2024-02-11_R_and_Python/index.html#with-conditionals",
    "title": "AR and Python side-by-side for data wrangling",
    "section": "With conditionals?",
    "text": "With conditionals?\n\n let’s do something slightly more complicated\n\ngapminder %&lt;&gt;% \n  mutate(group = case_when(\n    region %in% c(\n      \"Western Europe\", \"Northern Europe\",\"Southern Europe\", \n      \"Northern America\", \n      \"Australia and New Zealand\") ~ \"West\",\n    region %in% c(\n      \"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    region %in% c(\n      \"Caribbean\", \"Central America\", \n      \"South America\") ~ \"Latin America\",\n    continent == \"Africa\" & \n      region != \"Northern Africa\" ~ \"Sub-Saharan\",\n    TRUE ~ \"Others\"))\n\ngapminder |&gt; count(group)\n\n\n  \n\n\n\n\n\n# #| eval: false\n# gapminder = (gapminder.with_columns(\n#   pl.when(\n#     pl.col(\"region\").is_in([\n#       \"Western Europe\", \"Northern Europe\",\"Southern Europe\", \n#       \"Northern America\", \"Australia and New Zealand\"]))\n#     .then(\"West\")\n#     .when(\n#       pl.col(\"region\").is_in([\n#         \"Eastern Asia\", \"South-Eastern Asia\"]))\n#     .then(\"East Asia\")\n#     .when(\n#       pl.col(\"region\").is_in([\n#         \"Caribbean\", \"Central America\", \n#         \"South America\"]))\n#     .then(\"Latin America\")\n#     .when(\n#       (pl.col(\"continent\") == \"Africa\") & \n#       (pl.col(\"region\") != \"Northern Africa\"))\n#     .then(\"Sub-Saharan\")\n#     .otherwise(\"Other\")\n#     .alias(\"group\")\n# ))\n# \n# (gapminder\n#   .group_by(\"group\")\n#   .agg([ pl.count() ])\n#   .sort(\"group\")\n# )\n\n\n# #| echo: false\n# #| output:  true\n# df &lt;- py$df\n# df\n\nI think this is probably a good enough intro to how you’d generally do things. Filtering, and aggregatingare probably the most foundational and this could already get you started in another language without as much headache"
  },
  {
    "objectID": "blog/2024-02-11_R_and_Python/index.html#scatterplots",
    "href": "blog/2024-02-11_R_and_Python/index.html#scatterplots",
    "title": "R and Python side-by-side for data wrangling",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nI’m trying to strike a balance between dead basic plotly plots and some things you might want to do to make them look a little more the way you want. The great thing about customizing is that you can write functions to do specific things.  In some instances you can create simple functions or just save a list of values you want to recycle throughout.\n + plotly\n\nplotly_title &lt;- function(title, subtitle, ...) {\n  return(\n    list(\n      text = str_glue(\n        \"\n        &lt;b&gt;{title}&lt;/b&gt;\n        &lt;sup&gt;{subtitle}&lt;/sup&gt;\n        \"),\n      ...))\n}\n\nmargin &lt;- list(\n  t = 95,\n  r = 40,\n  b = 120,\n  l = 79)\n\ngapminder |&gt;\n  filter(year == 1962) |&gt;\n  plot_ly(\n    x = ~fertility, y = ~life_expectancy, \n    color = ~continent, colors = \"Set2\", \n    type = \"scatter\", mode = \"markers\",\n    hoverinfo = \"text\",\n    text = ~str_glue(\n      \"\n      &lt;b&gt;{country}&lt;/b&gt;&lt;br&gt;\n      Continent: &lt;b&gt;{continent}&lt;/b&gt;\n      Fertility: &lt;b&gt;{fertility}&lt;/b&gt;\n      Life Expectancy: &lt;b&gt;{life_expectancy}&lt;/b&gt;\n      \"),\n    marker = list(\n      size = 7\n    )) |&gt;\n  layout(\n    margin = margin,\n    title = plotly_title(\n      title = \"Scatterplot\",\n      subtitle = \"Life expectancy by fertility\",\n      x = 0,\n      xref = \"paper\")) |&gt;\n  config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Plotly rendering\n\n\n\nA quick note about having plotly work inside of the RStudio IDE–as of the time of this writing it isn’t very straightforward, i.e., not officially supported yet. The plot will open in a browser window and it’s fairly snappy. The good think is that on the reticulate side, knitting works! So this side was able to put all this together via rmarkdown when I started this post and Quarto now that I’m finishing this post (remember any  chunk will default to the knitr engine), so that’s pretty cool. We’re even using both renv and venv for both environments in the same file \n\n\n\n + plotly\n\ndef plotly_title(title, subtitle):\n  return(f\"&lt;b&gt;{title}&lt;/b&gt;&lt;br&gt;&lt;sup&gt;{subtitle}&lt;/sup&gt;\")\n\nmargin = dict(\n  t = 95,\n  r = 40,\n  b = 120,\n  l = 79)\n  \nconfig = {\"displayModeBar\": False}\n\n(px.scatter(\n  (gapminder.filter(pl.col(\"year\") == 1962).to_pandas()),\n  x = \"fertility\", y = \"life_expectancy\", color = \"continent\",\n  hover_name = \"country\",\n  color_discrete_sequence = px.colors.qualitative.Set2,\n  title = plotly_title(\n    title = \"Scatterplot\", \n    subtitle = \"Life expectancy by fertility\"),\n  opacity = .8, \n  template = \"plotly_white\") \n  .update_traces(\n    marker = dict(\n      size = 7))\n  .update_layout(\n    margin = margin)\n).show(config = config) \n\n                        \n                                            \n\n\n\nplotly expects a pandas DataFrame so we’re just using .to_pandas() to give it what it wants, but that doesn’t have to stop you from adding any filtering, summarizing, or aggregating before chaining the data into your viz."
  },
  {
    "objectID": "blog/2024-02-11_R_and_Python/index.html#conclusion",
    "href": "blog/2024-02-11_R_and_Python/index.html#conclusion",
    "title": "R and Python side-by-side for data wrangling",
    "section": "Conclusion",
    "text": "Conclusion\n\nHopefully this is helpful. Feel free to reach out with any feedback or questions."
  }
]