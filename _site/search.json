[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Talks\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Your R Code!\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nDavid Munoz Tord\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/2023-04-Deploy-your-R-code/index.html",
    "href": "talks/2023-04-Deploy-your-R-code/index.html",
    "title": "Deploy Your R Code!",
    "section": "",
    "text": "Full Slides\nPresentation Recording"
  },
  {
    "objectID": "index.html#fa-hand-peace-hi-im-david.-im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project-to-work-on",
    "href": "index.html#fa-hand-peace-hi-im-david.-im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project-to-work-on",
    "title": "munoztd0",
    "section": " Hi, I’m David. I’m a Data Scientist / R Developer always looking for a cool open-source project to work on !",
    "text": "Hi, I’m David. I’m a Data Scientist / R Developer always looking for a cool open-source project to work on !"
  },
  {
    "objectID": "talks copy/2018-12-east-la-r-users/index.html",
    "href": "talks copy/2018-12-east-la-r-users/index.html",
    "title": "Interactive Dashboards with shiny",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "vitae copy.html",
    "href": "vitae copy.html",
    "title": "Full CV",
    "section": "",
    "text": "I believe that the 21st century requires a more multifaceted toolkit if we hope to solve complex challenges–process and task oriented approaches balanced within a framework that fosters strategic thinking. That interdisciplinary relationships can be strengthened through active community participation–working toward something with the shared value of doing good.\n I bring a lot of energy and enthusiasm to the projects that I become involved in. I thrive in situations that require imagination and innovation. My interests are varied; among my passions I would list: academic inquiry, classical and contemporary thought, the music and cuisine of other cultures, the arts, traveling, languages, and social justice. I believe that our most vulnerable deserve respect and to be treated with dignity and fairness.\n Ultimately I hope that the work I do is helpful–that it enables discovery or provides clarity. I hope to bring together practical and principled approaches and I hope to further grow as this philosophy guides me.\n\n\n\n\n\n\n I am dedicated and hard working–a good communicator and active community participant. I work well on team efforts and projects with the ultimate aim of using data for social good.\n I am:\n\n creative\n honest\n friendly\n enthusiastic\n keen to learn new things and take on new roles\n reliable\n a lateral thinker\n motivated\n\n I am able to:\n\n speak and read French\n read and understand some German\n listen to and follow instructions accurately\n work cooperatively\n take on new challenges\n complete tasks\n support others\n identify new innovations and technologies while working toward implementing them\n be proactive\n\n\n\n\n\n\n\n Web Technologies: HTML, CSS, Javascript; Quarto publishing; Rmarkdown Websites, Dashboards in R with shiny/flexdashboard that support crosstalk, and interactive data visualization with Plotly; Static Site Generators, i.e., Pelican, Jekyll, and others; Traditional CMSs like Wordpress and Drupal; previous coursework in Information Architecture and UX/UI design.\n Coding: R, Python, and Javascript.\n OSs: macOS, Linux, Windows, and containers. Within the Linux ecosystem, I have experience with Debian based distributions as well as Red Hat Enterprise Linux. I love the terminal and wish vim bindings were universal.\n Data: Relational Databases (data modeling and SQL); Extract-Transfer-Load (ETL) and Extract-Load-Transfer (ELT) strategies for data engineering–specifically with Apache Airflow for ETL and Data Build Tool (dbt) for ELT; information retrieval; taxonomies and ontologies; and, knowledge organization theory.\n Data Science: both R and Python are a big part of my workflow from reading in data, transforming and ‘tidying’ data, exploring and visualizing data, modeling, reporting, presenting, and tying it all together in a dashboard. In connection to R, I am a proponent of the tidyverse framework for its unifying API. With respect to Python, I enjoy using polars, pyjanitor, pymc, and plotly to work in equivalent ways. I’m honestly happy working in both languages.\n Metadata schemas: knowledge of RDF, OWL, SPARQL, AACR2, RDA, MODS, METS, and PREMIS\n\n\n\n\n\n\n University of North Texas | Master of Science in Information Science (2013) Information Systems track\n University of California, Los Angeles (UCLA) | Bachelor of Arts in French and comparative literature (2008 / double major)\n Université de la Sorbonne Nouvelle (Paris III) | Student exchange (2006–2007)\n\n\n\n\n\n\n\n\n\n\n excited to bring my DevOps and Data Engineering skills to bear on thorny data problems in support of public good.\n\n\n Skylight is a digital government consultancy driven by making government work in a digital world through design, technology, and procurement.\n\n\n\n\n\n\n\n\n using expert knowledge of and experience in data, statistical modeling, and machine learning methods; programming languages, packages/libraries, and big-data engineering solutions; manage and implement highly technical and complex data science projects from defining the vision to operationalizing solutions to ensure desired outcomes are achieved.\n\n\n provides both technical and administrative supervision to the DSS team of four to five staff (on average) as we plan, develop, and implement data science projects, data engineering pipelines, and dashboards/visual analytics. The DSS team is comprised of two Data Scientists, a Jr. Data Scientist, a Data Engineer, and a Product Manager. We utilize the SCRUM framework to monitor and direct tasks, boards to communicate and evaluate progress, peer-to-peer code review in GitHub via pull request rules, and one on one sessions for pair programming and additional mentoring.\n\n\n work closely with both senior Public Health Information Systems (PHIS) IT staff and Internal Services Department (ISD) IT staff on a variety of data infrastructure projects including the management and maintenance of our data science tooling (both the applications and the servers); work strategically on data engineering bottlenecks to enhance reliability and scalability of data; use knowledge of data science and data engineering practices to add suggestions for burgeoning information management and governance work; act as technical resource for team leads interested in leveraging DSS tooling.\n\n\n building a robust R and Python community by collaborating closely with our RStudio partners to bring trainings to more than 180 staff within ACDC; helping to answer questions and support team members that run into issues related to version control, CI/CD, RStudio Connect, and other DSS tools; evangelize data modernization initiatives whenever possible while also staying up to date on the changing data science landscape.\n\n\n led DSS team to refactor (rewrite) from SAS and R codebases to large-scale (big data) architecture code that is automated with the Apache Airflow workflow tool leveraging Apache Spark via Python; these processes in some instances are three to four times faster than the original codebases, which we plan to continue to leverage for all of our Extract-Transfer-Load (ETL) needs.\n\n\n continuing to define and formalize the road map for data science within public health as an embedded group of expert resources that can consult and collaborate on a divisional or departmental opportunities/gaps in infrastructure or data engineering we can recommend plans for strengthening, streamlining, or augmenting.\n\n\n\n\n\n\n\n\n spearheading data integration projects with Whole Person Care program; optimizing links to internal master data management, countywide master data management, community/clinic based HIEs, and MCOs.\n\n\n\n help prototype operational dashboards for analyst team to put into production, working with clinical staff on creating tools that will add value rather than distract or oversaturate.\n\n\n work with leadership on identifying KPIs and other operationally valuable markers for high level dashboard applications that allow users to explore and interact adding business value.\n\n\n liaise with external county departments, HIEs, and MCOs regarding data sharing standards and governance for WPC population–up to speed on current federal and state regulations for what can and what cannot be shared depending on context.\n\n\n assist in authoring analysis billing rules for data team to use to correctly produce mandatory reporting given complexities of WPC funding via Medi-Cal waiver program and shifting horizon of competing and complimentary programming.\n\n\n responsible for calculating program milestones and incentives for state billing–working with front line program staff to improve workflows and capture the data the demonstrates achievement for the WPC program.\n\n\n act as bridge between IT, data team, evaluation team, and leadership on all data related matters–working to improve programming workflows through version control and software engineering best practices.\n\n\n\n\n\n\n\n\n provide strategic oversight to organization’s data collection systems and reporting processes through the collection and analysis of data, reporting on and capturing of outcome metrics, and providing input to create a more data-driven culture\n\n\n responsible for identifying, monitoring, and evaluating program and organizational outcomes to ensure compliance and effectiveness of programming\n\n\n moved social service database from a no access closed system to AWS to reduce organizational cost and optimize IT ecosystem\n\n\n liaised with internal departments for reporting/proposal questions, and to answer questions about resident population trends, funding and compliance layers for affordable housing; and, how the two work toward a broader outcomes strategy\n\n\n prepare recurring and special aggregated data reports and related decision support tools for organization to support analytics value chain: datareportinganalysisactionvalue\n\n\n oversaw the application and implementation of assessment procedures for longitudinal data collection; provided training in the use of survey instruments and answers to questions about what is tracked and why\n\n\n leveraged agglomerative clustering models, survival models, generalized linear models, and binary classifiers like logistic regression in order to build a wider understanding of how people fare in permanent supportive housing to see if we can detect negative outcomes early and target support services early.\n\n\n converted all data workflows to version control via GitHub for all projects including web application and some of the data engineering\n\n\n liaise with external organizations, research institutions, and contractors / consultants interested in or currently conducting research with organization to answer questions about what we currently are using to track outcomes / offer support\n\n\n\n\n\n\n\n\n provide in-depth analysis of both the resident population and underlying subsidy and compliance structure for entire portfolio\n\n\n leverage validated research tools to improve data collection; providing a pathway for future program evaluation based on real measurements that have reliable sensitivity and specificity\n\n\n explored open-source database alternatives to reduce organizational cost and fit within the IT ecosystem\n\n\n actively participated in internal organizational dialog around the need to pivot from demonstrating need to demonstrating efficacy–championed the role data could play in program development, intervention, and outcomes\n\n\n prepared recurring and special aggregated data reports for asset management compliance, annual organizational audit, resident programs grant reporting, and housing development RFP/RSFQ submissions\n\n\n\n assist in any data related projects with community partners and research institutions\n\n\n\n\n\n\n\n\n gathered baseline data on current resident enrollment in Medi-Cal, their health care utilization and access, history of chronic health conditions, as well as food intake.\n\n\n performed statistical analysis of health survey data using Python–we had 200 respondents and were able to make programmatic changes based on our findings\n\n\n developed and implemented health training workshops and on-on-one guidance for Program Managers and Resident Services Coordinators to improve knowledge of new Affordable Care Act-driven insurance enrollment challenges for residents with advanced conditions\n\n\n implemented resident-focused trainings on managing and seeking help for chronic health conditions as well as creating health literacy guides for residents\n\n\n planed and coordinated resident health fairs in collaboration with community health providers to improve health literacy of residents in connection to both chronic conditions as well as the resources in their community\n\n\n provided coordinating support to Program Managers and staff charged with oversight of existing on-site health resources and wellness programs. Explored potential partnerships to provide residents improved health and well-being\n\n\n prepared and maintained accurate reports and survey data utilizing the Efforts to Outcomes database\n\n\n project troubleshooter par excellence.\n\n\n\n\n\n\n\n\n responsible for administering and uploading survey data to the Substance Abuse and Mental Health Services Administration (SAMHSA) and its affiliates while maintaining confidentiality and records keeping due diligence\n\n\n worked with Resident Service Coordinators, programmatic staff, and grant evaluator on collecting and maintaining data pertinent mandated progress reports to SAMHSA\n\n\n researched, wrote, and compiled bi-annual reports for the Cooperative Agreement to Benefit Homeless Individuals (CABHI) grant project\n\n\n point-person/project manager for follow up and grant work-plan compliance with Skid Row Community Consortium\n\n\n researched and identified future survey instruments for targeted sub-populations to improve data collection practices\n\n\n participated in regular trainings, program evaluation, and program development\n\n\n designed and edited Peer Advocate zine for commercial printing\n\n\n improved client access to resources/information through small booklets tailored to the population\n\n\n\n\n\n\n\n\n designed and implemented new digital signage/print fliers to promote library outreach, services, and events\n\n\n created new social media presence for (then) new platform Vine to promote library collections and services\n\n\n designed online content that highlighted collections, incorporated teaching and learning services and provided wide audience appreciation\n\n\n facilitated presentation on approach and ideas to UCLA Library’s social media steering committee\n\n\n operated multi-tiered social media platform apparatus\n\n\n\n\n\n\n\n\n\n\n\n Researched and implemented a suite of validated research questionnaires to collect survey data, deployed to database for end users to input; tidied, transformed, and modeled over 900 responses; and built and hosted a Shiny dashboard to communicate results. To be a part of the process from beginning to end was a huge learning process and I see it as a great accomplishment.\n\n\n Designed, proofed, prepared, and labored over Peer Advocate ’Zine for Skid Row Housing Trust’s Hilton Project ’Zine “I Got You”, which will be released during a end of grant event for all project participants\n\n\n Was asked to help design and brainstorm a flier for the UCLA Library system because of interest in design and outreach for libraries–the final project was the “Top 10 Things” flier, which was a successful campaign to engage students in library programming and services\n\n\n Was recognized by two articles online for work promoting library services and collections through early-adoption of Vine\n\n\n\n\n\n Learning more about statistics and data science; particularly its applications in R. I am more and more becoming interested in Bayesian approaches.\n\n\n Coding: I will likely never code as neatly as an engineer but I try to incorporate software engineering best practices as I can–I am very keen to become more proficient with workflows in airflow, deploying APIs to AWS, and taking on more data engineering as practicable.\n\n\n Design: I love typefaces and modernist design, in another life I would love to work as a designer or artist so it is something I passionately admire and sometimes try to do when time permits.\n\n\n Exploration: I like to explore other places through travel and food; ideologies through reading and communicating, and world through science and discovery!\n\n\nUpdated on: r Sys.Date()"
  },
  {
    "objectID": "vitae copy.html#fa-compass-personal-philosophy",
    "href": "vitae copy.html#fa-compass-personal-philosophy",
    "title": "Full CV",
    "section": "",
    "text": "I believe that the 21st century requires a more multifaceted toolkit if we hope to solve complex challenges–process and task oriented approaches balanced within a framework that fosters strategic thinking. That interdisciplinary relationships can be strengthened through active community participation–working toward something with the shared value of doing good.\n I bring a lot of energy and enthusiasm to the projects that I become involved in. I thrive in situations that require imagination and innovation. My interests are varied; among my passions I would list: academic inquiry, classical and contemporary thought, the music and cuisine of other cultures, the arts, traveling, languages, and social justice. I believe that our most vulnerable deserve respect and to be treated with dignity and fairness.\n Ultimately I hope that the work I do is helpful–that it enables discovery or provides clarity. I hope to bring together practical and principled approaches and I hope to further grow as this philosophy guides me."
  },
  {
    "objectID": "vitae copy.html#fa-magic-personal-skills",
    "href": "vitae copy.html#fa-magic-personal-skills",
    "title": "Full CV",
    "section": "",
    "text": "I am dedicated and hard working–a good communicator and active community participant. I work well on team efforts and projects with the ultimate aim of using data for social good.\n I am:\n\n creative\n honest\n friendly\n enthusiastic\n keen to learn new things and take on new roles\n reliable\n a lateral thinker\n motivated\n\n I am able to:\n\n speak and read French\n read and understand some German\n listen to and follow instructions accurately\n work cooperatively\n take on new challenges\n complete tasks\n support others\n identify new innovations and technologies while working toward implementing them\n be proactive"
  },
  {
    "objectID": "vitae copy.html#fa-code-technical-skills",
    "href": "vitae copy.html#fa-code-technical-skills",
    "title": "Full CV",
    "section": "",
    "text": "Web Technologies: HTML, CSS, Javascript; Quarto publishing; Rmarkdown Websites, Dashboards in R with shiny/flexdashboard that support crosstalk, and interactive data visualization with Plotly; Static Site Generators, i.e., Pelican, Jekyll, and others; Traditional CMSs like Wordpress and Drupal; previous coursework in Information Architecture and UX/UI design.\n Coding: R, Python, and Javascript.\n OSs: macOS, Linux, Windows, and containers. Within the Linux ecosystem, I have experience with Debian based distributions as well as Red Hat Enterprise Linux. I love the terminal and wish vim bindings were universal.\n Data: Relational Databases (data modeling and SQL); Extract-Transfer-Load (ETL) and Extract-Load-Transfer (ELT) strategies for data engineering–specifically with Apache Airflow for ETL and Data Build Tool (dbt) for ELT; information retrieval; taxonomies and ontologies; and, knowledge organization theory.\n Data Science: both R and Python are a big part of my workflow from reading in data, transforming and ‘tidying’ data, exploring and visualizing data, modeling, reporting, presenting, and tying it all together in a dashboard. In connection to R, I am a proponent of the tidyverse framework for its unifying API. With respect to Python, I enjoy using polars, pyjanitor, pymc, and plotly to work in equivalent ways. I’m honestly happy working in both languages.\n Metadata schemas: knowledge of RDF, OWL, SPARQL, AACR2, RDA, MODS, METS, and PREMIS"
  },
  {
    "objectID": "vitae copy.html#fa-graduation-cap-education",
    "href": "vitae copy.html#fa-graduation-cap-education",
    "title": "Full CV",
    "section": "",
    "text": "University of North Texas | Master of Science in Information Science (2013) Information Systems track\n University of California, Los Angeles (UCLA) | Bachelor of Arts in French and comparative literature (2008 / double major)\n Université de la Sorbonne Nouvelle (Paris III) | Student exchange (2006–2007)"
  },
  {
    "objectID": "vitae copy.html#fa-institution-work-experience",
    "href": "vitae copy.html#fa-institution-work-experience",
    "title": "Full CV",
    "section": "",
    "text": "excited to bring my DevOps and Data Engineering skills to bear on thorny data problems in support of public good.\n\n\n Skylight is a digital government consultancy driven by making government work in a digital world through design, technology, and procurement.\n\n\n\n\n\n\n\n\n using expert knowledge of and experience in data, statistical modeling, and machine learning methods; programming languages, packages/libraries, and big-data engineering solutions; manage and implement highly technical and complex data science projects from defining the vision to operationalizing solutions to ensure desired outcomes are achieved.\n\n\n provides both technical and administrative supervision to the DSS team of four to five staff (on average) as we plan, develop, and implement data science projects, data engineering pipelines, and dashboards/visual analytics. The DSS team is comprised of two Data Scientists, a Jr. Data Scientist, a Data Engineer, and a Product Manager. We utilize the SCRUM framework to monitor and direct tasks, boards to communicate and evaluate progress, peer-to-peer code review in GitHub via pull request rules, and one on one sessions for pair programming and additional mentoring.\n\n\n work closely with both senior Public Health Information Systems (PHIS) IT staff and Internal Services Department (ISD) IT staff on a variety of data infrastructure projects including the management and maintenance of our data science tooling (both the applications and the servers); work strategically on data engineering bottlenecks to enhance reliability and scalability of data; use knowledge of data science and data engineering practices to add suggestions for burgeoning information management and governance work; act as technical resource for team leads interested in leveraging DSS tooling.\n\n\n building a robust R and Python community by collaborating closely with our RStudio partners to bring trainings to more than 180 staff within ACDC; helping to answer questions and support team members that run into issues related to version control, CI/CD, RStudio Connect, and other DSS tools; evangelize data modernization initiatives whenever possible while also staying up to date on the changing data science landscape.\n\n\n led DSS team to refactor (rewrite) from SAS and R codebases to large-scale (big data) architecture code that is automated with the Apache Airflow workflow tool leveraging Apache Spark via Python; these processes in some instances are three to four times faster than the original codebases, which we plan to continue to leverage for all of our Extract-Transfer-Load (ETL) needs.\n\n\n continuing to define and formalize the road map for data science within public health as an embedded group of expert resources that can consult and collaborate on a divisional or departmental opportunities/gaps in infrastructure or data engineering we can recommend plans for strengthening, streamlining, or augmenting.\n\n\n\n\n\n\n\n\n spearheading data integration projects with Whole Person Care program; optimizing links to internal master data management, countywide master data management, community/clinic based HIEs, and MCOs.\n\n\n\n help prototype operational dashboards for analyst team to put into production, working with clinical staff on creating tools that will add value rather than distract or oversaturate.\n\n\n work with leadership on identifying KPIs and other operationally valuable markers for high level dashboard applications that allow users to explore and interact adding business value.\n\n\n liaise with external county departments, HIEs, and MCOs regarding data sharing standards and governance for WPC population–up to speed on current federal and state regulations for what can and what cannot be shared depending on context.\n\n\n assist in authoring analysis billing rules for data team to use to correctly produce mandatory reporting given complexities of WPC funding via Medi-Cal waiver program and shifting horizon of competing and complimentary programming.\n\n\n responsible for calculating program milestones and incentives for state billing–working with front line program staff to improve workflows and capture the data the demonstrates achievement for the WPC program.\n\n\n act as bridge between IT, data team, evaluation team, and leadership on all data related matters–working to improve programming workflows through version control and software engineering best practices.\n\n\n\n\n\n\n\n\n provide strategic oversight to organization’s data collection systems and reporting processes through the collection and analysis of data, reporting on and capturing of outcome metrics, and providing input to create a more data-driven culture\n\n\n responsible for identifying, monitoring, and evaluating program and organizational outcomes to ensure compliance and effectiveness of programming\n\n\n moved social service database from a no access closed system to AWS to reduce organizational cost and optimize IT ecosystem\n\n\n liaised with internal departments for reporting/proposal questions, and to answer questions about resident population trends, funding and compliance layers for affordable housing; and, how the two work toward a broader outcomes strategy\n\n\n prepare recurring and special aggregated data reports and related decision support tools for organization to support analytics value chain: datareportinganalysisactionvalue\n\n\n oversaw the application and implementation of assessment procedures for longitudinal data collection; provided training in the use of survey instruments and answers to questions about what is tracked and why\n\n\n leveraged agglomerative clustering models, survival models, generalized linear models, and binary classifiers like logistic regression in order to build a wider understanding of how people fare in permanent supportive housing to see if we can detect negative outcomes early and target support services early.\n\n\n converted all data workflows to version control via GitHub for all projects including web application and some of the data engineering\n\n\n liaise with external organizations, research institutions, and contractors / consultants interested in or currently conducting research with organization to answer questions about what we currently are using to track outcomes / offer support\n\n\n\n\n\n\n\n\n provide in-depth analysis of both the resident population and underlying subsidy and compliance structure for entire portfolio\n\n\n leverage validated research tools to improve data collection; providing a pathway for future program evaluation based on real measurements that have reliable sensitivity and specificity\n\n\n explored open-source database alternatives to reduce organizational cost and fit within the IT ecosystem\n\n\n actively participated in internal organizational dialog around the need to pivot from demonstrating need to demonstrating efficacy–championed the role data could play in program development, intervention, and outcomes\n\n\n prepared recurring and special aggregated data reports for asset management compliance, annual organizational audit, resident programs grant reporting, and housing development RFP/RSFQ submissions\n\n\n\n assist in any data related projects with community partners and research institutions\n\n\n\n\n\n\n\n\n gathered baseline data on current resident enrollment in Medi-Cal, their health care utilization and access, history of chronic health conditions, as well as food intake.\n\n\n performed statistical analysis of health survey data using Python–we had 200 respondents and were able to make programmatic changes based on our findings\n\n\n developed and implemented health training workshops and on-on-one guidance for Program Managers and Resident Services Coordinators to improve knowledge of new Affordable Care Act-driven insurance enrollment challenges for residents with advanced conditions\n\n\n implemented resident-focused trainings on managing and seeking help for chronic health conditions as well as creating health literacy guides for residents\n\n\n planed and coordinated resident health fairs in collaboration with community health providers to improve health literacy of residents in connection to both chronic conditions as well as the resources in their community\n\n\n provided coordinating support to Program Managers and staff charged with oversight of existing on-site health resources and wellness programs. Explored potential partnerships to provide residents improved health and well-being\n\n\n prepared and maintained accurate reports and survey data utilizing the Efforts to Outcomes database\n\n\n project troubleshooter par excellence.\n\n\n\n\n\n\n\n\n responsible for administering and uploading survey data to the Substance Abuse and Mental Health Services Administration (SAMHSA) and its affiliates while maintaining confidentiality and records keeping due diligence\n\n\n worked with Resident Service Coordinators, programmatic staff, and grant evaluator on collecting and maintaining data pertinent mandated progress reports to SAMHSA\n\n\n researched, wrote, and compiled bi-annual reports for the Cooperative Agreement to Benefit Homeless Individuals (CABHI) grant project\n\n\n point-person/project manager for follow up and grant work-plan compliance with Skid Row Community Consortium\n\n\n researched and identified future survey instruments for targeted sub-populations to improve data collection practices\n\n\n participated in regular trainings, program evaluation, and program development\n\n\n designed and edited Peer Advocate zine for commercial printing\n\n\n improved client access to resources/information through small booklets tailored to the population\n\n\n\n\n\n\n\n\n designed and implemented new digital signage/print fliers to promote library outreach, services, and events\n\n\n created new social media presence for (then) new platform Vine to promote library collections and services\n\n\n designed online content that highlighted collections, incorporated teaching and learning services and provided wide audience appreciation\n\n\n facilitated presentation on approach and ideas to UCLA Library’s social media steering committee\n\n\n operated multi-tiered social media platform apparatus"
  },
  {
    "objectID": "vitae copy.html#fa-trophy-accomplishments",
    "href": "vitae copy.html#fa-trophy-accomplishments",
    "title": "Full CV",
    "section": "",
    "text": "Researched and implemented a suite of validated research questionnaires to collect survey data, deployed to database for end users to input; tidied, transformed, and modeled over 900 responses; and built and hosted a Shiny dashboard to communicate results. To be a part of the process from beginning to end was a huge learning process and I see it as a great accomplishment.\n\n\n Designed, proofed, prepared, and labored over Peer Advocate ’Zine for Skid Row Housing Trust’s Hilton Project ’Zine “I Got You”, which will be released during a end of grant event for all project participants\n\n\n Was asked to help design and brainstorm a flier for the UCLA Library system because of interest in design and outreach for libraries–the final project was the “Top 10 Things” flier, which was a successful campaign to engage students in library programming and services\n\n\n Was recognized by two articles online for work promoting library services and collections through early-adoption of Vine"
  },
  {
    "objectID": "vitae copy.html#fa-child-interests",
    "href": "vitae copy.html#fa-child-interests",
    "title": "Full CV",
    "section": "",
    "text": "Learning more about statistics and data science; particularly its applications in R. I am more and more becoming interested in Bayesian approaches.\n\n\n Coding: I will likely never code as neatly as an engineer but I try to incorporate software engineering best practices as I can–I am very keen to become more proficient with workflows in airflow, deploying APIs to AWS, and taking on more data engineering as practicable.\n\n\n Design: I love typefaces and modernist design, in another life I would love to work as a designer or artist so it is something I passionately admire and sometimes try to do when time permits.\n\n\n Exploration: I like to explore other places through travel and food; ideologies through reading and communicating, and world through science and discovery!\n\n\nUpdated on: r Sys.Date()"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Blog\n\n\n\n\n\n\n\n\n\n\n\n\nA Computer Science Roadmap\n\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\nVestin Hategekimana\n\n\n\n\n\n\n\n\nYour first chat bot with python!\n\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n\n\nR Lunches in university of Geneva\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\n\n\n\n\n\n\nWe-Data Live on YouTube\n\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\nData Manipulation with R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\nData Exploration with R\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\nSatistics with R\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\nCode Hygiene. Don’t Laugh it off !\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\nData Viz Fundamentals with R\n\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n\n\n\n\n\n\nLearn R Basics\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks copy/2017-09-Data-and-Donuts/index.html",
    "href": "talks copy/2017-09-Data-and-Donuts/index.html",
    "title": "Ex-nihilo Data Analysis; Getting up and Running with Limited Resources",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "talks copy/2018-09-ucla-cpi/index.html",
    "href": "talks copy/2018-09-ucla-cpi/index.html",
    "title": "Thoughts and Perspectives on PSH from the Ground Up",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\n\n\nEcharts4r\n\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2022\n\n\nDavid Munoz Tord\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2020-04-covid19-dashboard/index.html",
    "href": "projects/2020-04-covid19-dashboard/index.html",
    "title": "Echarts4r",
    "section": "",
    "text": "Full Screen\n\n\n\nDescription\n\n\n\nThis R package is a wrapper of the ECharts library. It provides a set of functions to generate interactive charts in R.  Highlights of this package:  - Provide functions to generate interactive charts in R. - The package is built on top of the ECharts library, which is a powerful library for data visualization. - The package provides a set of functions that can be used to generate charts with ECharts. - The package is still under development. More features will be added in the future. - The package is open source.   Features of the package: \n\nProvides more than 20 chart types available out of the box, along with a dozen components, and each of them can be arbitrarily combined to use.\nHas a powerful rendering engine that allows you to easily switch between Canvas and SVG rendering. Progressive rendering and stream loading make it possible to render 10 million data in realtime.\nOffers professional data analysis through datasets, which support data transforms like filtering, clustering, and regression to help analyze multi-dimensional analysis of the same data.\nHas an elegant visual design that follows visualization principles and supports responsive design. Flexible configurations make it easy to customize.\nHas a healthy community that ensures the healthy development of the project and contributes a wealth of third-party extensions.\nIs accessibility-friendly with automatically generated chart descriptions and decal patterns that help users with disabilities understand the content and the stories behind the charts.\n\n\nYou can check the project on Github."
  },
  {
    "objectID": "blog/Code_Hygiene/index.html",
    "href": "blog/Code_Hygiene/index.html",
    "title": "Code Hygiene. Don’t Laugh it off !",
    "section": "",
    "text": "By Fabrice Hategekimana\nGoal: Introduce the basics of hygiene in the code to have clean and reusable scripts. Workflow and refactoring are also important elements to achieve good results. We explain everything in our videos!\nPlaylist here\nNote: video in french, ask in comments for subtitle in your language"
  },
  {
    "objectID": "blog/Code_Hygiene/index.html#first-vidéo-of-the-playlist",
    "href": "blog/Code_Hygiene/index.html#first-vidéo-of-the-playlist",
    "title": "Code Hygiene. Don’t Laugh it off !",
    "section": "First vidéo of the playlist:",
    "text": "First vidéo of the playlist:"
  },
  {
    "objectID": "blog/Stats_With_R/index.html",
    "href": "blog/Stats_With_R/index.html",
    "title": "Satistics with R",
    "section": "",
    "text": "By Vestin Hategekimana\nGoal: To cover the structure of a classical statistical analysis with R. We will then go into more depth on each part while presenting packages in the language that make the work easier. Although this playlist is designed for social science students, it can be useful for anyone.\nPlaylist here\nNote: video in french, ask in comments for subtitle in your language"
  },
  {
    "objectID": "blog/Stats_With_R/index.html#first-video-of-the-playlist",
    "href": "blog/Stats_With_R/index.html#first-video-of-the-playlist",
    "title": "Satistics with R",
    "section": "First video of the playlist:",
    "text": "First video of the playlist:"
  },
  {
    "objectID": "blog/Live/index.html",
    "href": "blog/Live/index.html",
    "title": "We-Data Live on YouTube",
    "section": "",
    "text": "By David Munoz Tord, Fabrice Hategekimana and Vestin Hategekimana\nLive of December 29 edited in which we present in more detail WeData, its functioning and its future.\nNote: video in french, ask in comments for subtitle in your language\nVideo link"
  },
  {
    "objectID": "blog/R_basics/index.html",
    "href": "blog/R_basics/index.html",
    "title": "Learn R Basics",
    "section": "",
    "text": "By Vestin Hategekimana\nGoal: To initiate people who would like to start with R by starting with the basics or the basics or people who want to reinforce their knowledge by going through important concepts in programming.\nPlaylist here\nNote: video in french, ask in comments for subtitle in your language"
  },
  {
    "objectID": "blog/R_basics/index.html#first-vidéo-of-the-playlist",
    "href": "blog/R_basics/index.html#first-vidéo-of-the-playlist",
    "title": "Learn R Basics",
    "section": "First vidéo of the playlist:",
    "text": "First vidéo of the playlist:"
  },
  {
    "objectID": "blog/Coding_road_map/Index.html",
    "href": "blog/Coding_road_map/Index.html",
    "title": "A Computer Science Roadmap",
    "section": "",
    "text": "A Computer Science Roadmap: From Theory to Mastery\nHey there, future coding maestro! Venturing into the vast realm of computer science can seem intimidating. Fear not! This roadmap will guide you through theoretical foundations, essential languages, and core software engineering practices, all while encouraging you to explore further.\nNote: It’s clearly an exaggeration to sum up all computer science learning in a single post. In reality, each part deserves its own roadmap. But for the moment we’re content with this because of the structure of our channel. When we get more content on our YouTube channel, we’ll be making a lot of changes to this roadmap. So, don’t see this content as the entirety of what you need to learn, but simply as the beginning of a journey under construction!\n\n\n1. Theoretical Foundations\nBefore diving into coding, understanding the basics will serve you well throughout your journey.\n\nAlgorithms & Data Structures: Learn how algorithms work and why certain data structures are preferred for specific tasks.\n\n\n\n\n2. Introduction to Shell Programming\nThe command-line interface (CLI) is a powerful tool.\nBasic\n\nBasics of CLI: Navigate directories, manage files, and get comfortable with the terminal.\n\n\nTerminal playlist\n\nShell Scripting: Automate tasks using bash (or your shell of choice) scripting.\n\n\n\nModern Shell: Let’s discover some modern shell and their interesting features. Here we dive into Nushell!\n\n\nNushell playlist\n\n\n\n3. Python\nA versatile language with a gentle learning curve.\n\nPython Basics: Variables, loops, conditions, and functions.\n\n\nPython’s fundamental playlist\n\nAdvanced Python: Object-oriented programming, list comprehensions, and modules.\nPython Ecosystem: Libraries like numpy, pandas, and flask to supercharge your projects.\n\n\n\n\n4. Rust\nA language focusing on performance and safety.\n\nRust Fundamentals: Understand ownership, borrowing, and lifetimes.\n\n\nRust playlist\n\nConcurrency in Rust: Explore Rust’s approach to threads and safe concurrent programming.\n\n\n\n\n5. Haskell\nDive into the world of functional programming.\n\nHaskell Basics: Grasp the idea of pure functions, immutability, and laziness.\nFunctional Paradigms: Monads, functors, and lambdas.\n\n\n\n\n6. Go (Golang)\nSimplicity and efficiency in one package.\n\nGo Essentials: Learn the straightforward syntax, Goroutines, and channels.\nWeb with Go: Build robust web applications using Go’s standard library.\n\n\n\n\n7. C++\nA high-performance language used in system/software development.\n\nC++ Fundamentals: Classes, objects, inheritance, and polymorphism.\nSTL: Dive deep into the Standard Template Library and its offerings.\n\n\n\n\n8. JavaScript\nThe backbone of the modern web.\n\nJavaScript Basics: Understand variables, functions, and DOM manipulation.\nModern JavaScript: Explore ES6 and beyond. Asynchronous operations, Promises, and Fetch API.\nFrameworks: Get to know popular frameworks like React, Vue, and Angular.\n\n\n\n\n9. Software Engineering\nBeyond coding, there’s the art of building software.\n\nVersion Control: Get proficient with Git.\nDesign Patterns: Understand common software design patterns and their applications.\n\n\nSoftware engineering playlist\n\nTesting: Learn the importance of testing and various methodologies, from unit tests to integration tests.\n\n\nClean code playlist\n\n\n\n10. Vim\nThe quintessential text editor.\n\nVim Basics: Understand modes, basic navigation, and editing commands.\n\n\nVim playlist\n\nAdvanced Vim: Buffers, windows, plugins, and custom configurations.\n\n\n\n\n11. Exploring New Programming Languages\nNever stop learning!\n\nResearch: Look for emerging languages and their use cases.\n\n\nCode news playlist\n\nExperiment: Play with sample projects, build simple applications, and test their potential. It doesn’t matter if you’re ignorant to begin with - the most important thing is to discover new things.\n\n\nIgnorant videos playlist\n\n\n\nFinal Words\nRemember, the world of computer science is vast, and this roadmap is just the beginning. Trust in your ability to learn and grow. Every error, every compiled code, and every successfully deployed application is a step forward.\nHappy coding and may your journey through computer science be filled with wonder!"
  },
  {
    "objectID": "blog/R-Lunches/index.html",
    "href": "blog/R-Lunches/index.html",
    "title": "R Lunches in university of Geneva",
    "section": "",
    "text": "By David Munoz Tord\nR lunches are multidisciplinary meetings on R at UniMail.\nWe finished this semester R Lunches but you can still find the video links if you missed one!\nRead more about it"
  },
  {
    "objectID": "blog/Data_explor_Fundamentals/index.html",
    "href": "blog/Data_explor_Fundamentals/index.html",
    "title": "Data Exploration with R",
    "section": "",
    "text": "By David Munoz Tord\nGoal: Learn about data exploration and familiarize yourself with some of the basic functions of the tidyverse.\nOpen it full\nNote: All in english"
  },
  {
    "objectID": "projects/2022-12-echarts4r/index.html",
    "href": "projects/2022-12-echarts4r/index.html",
    "title": "Echarts4r",
    "section": "",
    "text": "Full Screen\n\n\n\nDescription\n\n\n\nThis R package is a wrapper of the ECharts library. It provides a set of functions to generate interactive charts in R.  Highlights of this package:  - Provide functions to generate interactive charts in R. - The package is built on top of the ECharts library, which is a powerful library for data visualization. - The package provides a set of functions that can be used to generate charts with ECharts. - The package is still under development. More features will be added in the future. - The package is open source.   Features of the package: \n\nProvides more than 20 chart types available out of the box, along with a dozen components, and each of them can be arbitrarily combined to use.\nHas a powerful rendering engine that allows you to easily switch between Canvas and SVG rendering. Progressive rendering and stream loading make it possible to render 10 million data in realtime.\nOffers professional data analysis through datasets, which support data transforms like filtering, clustering, and regression to help analyze multi-dimensional analysis of the same data.\nHas an elegant visual design that follows visualization principles and supports responsive design. Flexible configurations make it easy to customize.\nHas a healthy community that ensures the healthy development of the project and contributes a wealth of third-party extensions.\nIs accessibility-friendly with automatically generated chart descriptions and decal patterns that help users with disabilities understand the content and the stories behind the charts.\n\n\nYou can check the project on Github."
  }
]